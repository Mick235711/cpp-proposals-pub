<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>P1674R2&colon; Evolving a Standard C&plus;&plus; Linear Algebra Library from the BLAS</title>
        <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

/* From extension vscode.markdown-math */
@font-face{font-family:KaTeX_AMS;font-style:normal;font-weight:400;src:url(fonts/KaTeX_AMS-Regular.woff2) format("woff2"),url(fonts/KaTeX_AMS-Regular.woff) format("woff"),url(fonts/KaTeX_AMS-Regular.ttf) format("truetype")}@font-face{font-family:KaTeX_Caligraphic;font-style:normal;font-weight:700;src:url(fonts/KaTeX_Caligraphic-Bold.woff2) format("woff2"),url(fonts/KaTeX_Caligraphic-Bold.woff) format("woff"),url(fonts/KaTeX_Caligraphic-Bold.ttf) format("truetype")}@font-face{font-family:KaTeX_Caligraphic;font-style:normal;font-weight:400;src:url(fonts/KaTeX_Caligraphic-Regular.woff2) format("woff2"),url(fonts/KaTeX_Caligraphic-Regular.woff) format("woff"),url(fonts/KaTeX_Caligraphic-Regular.ttf) format("truetype")}@font-face{font-family:KaTeX_Fraktur;font-style:normal;font-weight:700;src:url(fonts/KaTeX_Fraktur-Bold.woff2) format("woff2"),url(fonts/KaTeX_Fraktur-Bold.woff) format("woff"),url(fonts/KaTeX_Fraktur-Bold.ttf) format("truetype")}@font-face{font-family:KaTeX_Fraktur;font-style:normal;font-weight:400;src:url(fonts/KaTeX_Fraktur-Regular.woff2) format("woff2"),url(fonts/KaTeX_Fraktur-Regular.woff) format("woff"),url(fonts/KaTeX_Fraktur-Regular.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:normal;font-weight:700;src:url(fonts/KaTeX_Main-Bold.woff2) format("woff2"),url(fonts/KaTeX_Main-Bold.woff) format("woff"),url(fonts/KaTeX_Main-Bold.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:italic;font-weight:700;src:url(fonts/KaTeX_Main-BoldItalic.woff2) format("woff2"),url(fonts/KaTeX_Main-BoldItalic.woff) format("woff"),url(fonts/KaTeX_Main-BoldItalic.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:italic;font-weight:400;src:url(fonts/KaTeX_Main-Italic.woff2) format("woff2"),url(fonts/KaTeX_Main-Italic.woff) format("woff"),url(fonts/KaTeX_Main-Italic.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:normal;font-weight:400;src:url(fonts/KaTeX_Main-Regular.woff2) format("woff2"),url(fonts/KaTeX_Main-Regular.woff) format("woff"),url(fonts/KaTeX_Main-Regular.ttf) format("truetype")}@font-face{font-family:KaTeX_Math;font-style:italic;font-weight:700;src:url(fonts/KaTeX_Math-BoldItalic.woff2) format("woff2"),url(fonts/KaTeX_Math-BoldItalic.woff) format("woff"),url(fonts/KaTeX_Math-BoldItalic.ttf) format("truetype")}@font-face{font-family:KaTeX_Math;font-style:italic;font-weight:400;src:url(fonts/KaTeX_Math-Italic.woff2) format("woff2"),url(fonts/KaTeX_Math-Italic.woff) format("woff"),url(fonts/KaTeX_Math-Italic.ttf) format("truetype")}@font-face{font-family:"KaTeX_SansSerif";font-style:normal;font-weight:700;src:url(fonts/KaTeX_SansSerif-Bold.woff2) format("woff2"),url(fonts/KaTeX_SansSerif-Bold.woff) format("woff"),url(fonts/KaTeX_SansSerif-Bold.ttf) format("truetype")}@font-face{font-family:"KaTeX_SansSerif";font-style:italic;font-weight:400;src:url(fonts/KaTeX_SansSerif-Italic.woff2) format("woff2"),url(fonts/KaTeX_SansSerif-Italic.woff) format("woff"),url(fonts/KaTeX_SansSerif-Italic.ttf) format("truetype")}@font-face{font-family:"KaTeX_SansSerif";font-style:normal;font-weight:400;src:url(fonts/KaTeX_SansSerif-Regular.woff2) format("woff2"),url(fonts/KaTeX_SansSerif-Regular.woff) format("woff"),url(fonts/KaTeX_SansSerif-Regular.ttf) format("truetype")}@font-face{font-family:KaTeX_Script;font-style:normal;font-weight:400;src:url(fonts/KaTeX_Script-Regular.woff2) format("woff2"),url(fonts/KaTeX_Script-Regular.woff) format("woff"),url(fonts/KaTeX_Script-Regular.ttf) format("truetype")}@font-face{font-family:KaTeX_Size1;font-style:normal;font-weight:400;src:url(fonts/KaTeX_Size1-Regular.woff2) format("woff2"),url(fonts/KaTeX_Size1-Regular.woff) format("woff"),url(fonts/KaTeX_Size1-Regular.ttf) format("truetype")}@font-face{font-family:KaTeX_Size2;font-style:normal;font-weight:400;src:url(fonts/KaTeX_Size2-Regular.woff2) format("woff2"),url(fonts/KaTeX_Size2-Regular.woff) format("woff"),url(fonts/KaTeX_Size2-Regular.ttf) format("truetype")}@font-face{font-family:KaTeX_Size3;font-style:normal;font-weight:400;src:url(fonts/KaTeX_Size3-Regular.woff2) format("woff2"),url(fonts/KaTeX_Size3-Regular.woff) format("woff"),url(fonts/KaTeX_Size3-Regular.ttf) format("truetype")}@font-face{font-family:KaTeX_Size4;font-style:normal;font-weight:400;src:url(fonts/KaTeX_Size4-Regular.woff2) format("woff2"),url(fonts/KaTeX_Size4-Regular.woff) format("woff"),url(fonts/KaTeX_Size4-Regular.ttf) format("truetype")}@font-face{font-family:KaTeX_Typewriter;font-style:normal;font-weight:400;src:url(fonts/KaTeX_Typewriter-Regular.woff2) format("woff2"),url(fonts/KaTeX_Typewriter-Regular.woff) format("woff"),url(fonts/KaTeX_Typewriter-Regular.ttf) format("truetype")}.katex{text-rendering:auto;font:normal 1.21em KaTeX_Main,Times New Roman,serif;line-height:1.2;text-indent:0}.katex *{-ms-high-contrast-adjust:none!important;border-color:currentColor}.katex .katex-version:after{content:"0.13.24"}.katex .katex-mathml{clip:rect(1px,1px,1px,1px);border:0;height:1px;overflow:hidden;padding:0;position:absolute;width:1px}.katex .katex-html>.newline{display:block}.katex .base{position:relative;white-space:nowrap;width:-webkit-min-content;width:-moz-min-content;width:min-content}.katex .base,.katex .strut{display:inline-block}.katex .textbf{font-weight:700}.katex .textit{font-style:italic}.katex .textrm{font-family:KaTeX_Main}.katex .textsf{font-family:KaTeX_SansSerif}.katex .texttt{font-family:KaTeX_Typewriter}.katex .mathnormal{font-family:KaTeX_Math;font-style:italic}.katex .mathit{font-family:KaTeX_Main;font-style:italic}.katex .mathrm{font-style:normal}.katex .mathbf{font-family:KaTeX_Main;font-weight:700}.katex .boldsymbol{font-family:KaTeX_Math;font-style:italic;font-weight:700}.katex .amsrm,.katex .mathbb,.katex .textbb{font-family:KaTeX_AMS}.katex .mathcal{font-family:KaTeX_Caligraphic}.katex .mathfrak,.katex .textfrak{font-family:KaTeX_Fraktur}.katex .mathtt{font-family:KaTeX_Typewriter}.katex .mathscr,.katex .textscr{font-family:KaTeX_Script}.katex .mathsf,.katex .textsf{font-family:KaTeX_SansSerif}.katex .mathboldsf,.katex .textboldsf{font-family:KaTeX_SansSerif;font-weight:700}.katex .mathitsf,.katex .textitsf{font-family:KaTeX_SansSerif;font-style:italic}.katex .mainrm{font-family:KaTeX_Main;font-style:normal}.katex .vlist-t{border-collapse:collapse;display:inline-table;table-layout:fixed}.katex .vlist-r{display:table-row}.katex .vlist{display:table-cell;position:relative;vertical-align:bottom}.katex .vlist>span{display:block;height:0;position:relative}.katex .vlist>span>span{display:inline-block}.katex .vlist>span>.pstrut{overflow:hidden;width:0}.katex .vlist-t2{margin-right:-2px}.katex .vlist-s{display:table-cell;font-size:1px;min-width:2px;vertical-align:bottom;width:2px}.katex .vbox{align-items:baseline;display:inline-flex;flex-direction:column}.katex .hbox{width:100%}.katex .hbox,.katex .thinbox{display:inline-flex;flex-direction:row}.katex .thinbox{max-width:0;width:0}.katex .msupsub{text-align:left}.katex .mfrac>span>span{text-align:center}.katex .mfrac .frac-line{border-bottom-style:solid;display:inline-block;width:100%}.katex .hdashline,.katex .hline,.katex .mfrac .frac-line,.katex .overline .overline-line,.katex .rule,.katex .underline .underline-line{min-height:1px}.katex .mspace{display:inline-block}.katex .clap,.katex .llap,.katex .rlap{position:relative;width:0}.katex .clap>.inner,.katex .llap>.inner,.katex .rlap>.inner{position:absolute}.katex .clap>.fix,.katex .llap>.fix,.katex .rlap>.fix{display:inline-block}.katex .llap>.inner{right:0}.katex .clap>.inner,.katex .rlap>.inner{left:0}.katex .clap>.inner>span{margin-left:-50%;margin-right:50%}.katex .rule{border:0 solid;display:inline-block;position:relative}.katex .hline,.katex .overline .overline-line,.katex .underline .underline-line{border-bottom-style:solid;display:inline-block;width:100%}.katex .hdashline{border-bottom-style:dashed;display:inline-block;width:100%}.katex .sqrt>.root{margin-left:.27777778em;margin-right:-.55555556em}.katex .fontsize-ensurer.reset-size1.size1,.katex .sizing.reset-size1.size1{font-size:1em}.katex .fontsize-ensurer.reset-size1.size2,.katex .sizing.reset-size1.size2{font-size:1.2em}.katex .fontsize-ensurer.reset-size1.size3,.katex .sizing.reset-size1.size3{font-size:1.4em}.katex .fontsize-ensurer.reset-size1.size4,.katex .sizing.reset-size1.size4{font-size:1.6em}.katex .fontsize-ensurer.reset-size1.size5,.katex .sizing.reset-size1.size5{font-size:1.8em}.katex .fontsize-ensurer.reset-size1.size6,.katex .sizing.reset-size1.size6{font-size:2em}.katex .fontsize-ensurer.reset-size1.size7,.katex .sizing.reset-size1.size7{font-size:2.4em}.katex .fontsize-ensurer.reset-size1.size8,.katex .sizing.reset-size1.size8{font-size:2.88em}.katex .fontsize-ensurer.reset-size1.size9,.katex .sizing.reset-size1.size9{font-size:3.456em}.katex .fontsize-ensurer.reset-size1.size10,.katex .sizing.reset-size1.size10{font-size:4.148em}.katex .fontsize-ensurer.reset-size1.size11,.katex .sizing.reset-size1.size11{font-size:4.976em}.katex .fontsize-ensurer.reset-size2.size1,.katex .sizing.reset-size2.size1{font-size:.83333333em}.katex .fontsize-ensurer.reset-size2.size2,.katex .sizing.reset-size2.size2{font-size:1em}.katex .fontsize-ensurer.reset-size2.size3,.katex .sizing.reset-size2.size3{font-size:1.16666667em}.katex .fontsize-ensurer.reset-size2.size4,.katex .sizing.reset-size2.size4{font-size:1.33333333em}.katex .fontsize-ensurer.reset-size2.size5,.katex .sizing.reset-size2.size5{font-size:1.5em}.katex .fontsize-ensurer.reset-size2.size6,.katex .sizing.reset-size2.size6{font-size:1.66666667em}.katex .fontsize-ensurer.reset-size2.size7,.katex .sizing.reset-size2.size7{font-size:2em}.katex .fontsize-ensurer.reset-size2.size8,.katex .sizing.reset-size2.size8{font-size:2.4em}.katex .fontsize-ensurer.reset-size2.size9,.katex .sizing.reset-size2.size9{font-size:2.88em}.katex .fontsize-ensurer.reset-size2.size10,.katex .sizing.reset-size2.size10{font-size:3.45666667em}.katex .fontsize-ensurer.reset-size2.size11,.katex .sizing.reset-size2.size11{font-size:4.14666667em}.katex .fontsize-ensurer.reset-size3.size1,.katex .sizing.reset-size3.size1{font-size:.71428571em}.katex .fontsize-ensurer.reset-size3.size2,.katex .sizing.reset-size3.size2{font-size:.85714286em}.katex .fontsize-ensurer.reset-size3.size3,.katex .sizing.reset-size3.size3{font-size:1em}.katex .fontsize-ensurer.reset-size3.size4,.katex .sizing.reset-size3.size4{font-size:1.14285714em}.katex .fontsize-ensurer.reset-size3.size5,.katex .sizing.reset-size3.size5{font-size:1.28571429em}.katex .fontsize-ensurer.reset-size3.size6,.katex .sizing.reset-size3.size6{font-size:1.42857143em}.katex .fontsize-ensurer.reset-size3.size7,.katex .sizing.reset-size3.size7{font-size:1.71428571em}.katex .fontsize-ensurer.reset-size3.size8,.katex .sizing.reset-size3.size8{font-size:2.05714286em}.katex .fontsize-ensurer.reset-size3.size9,.katex .sizing.reset-size3.size9{font-size:2.46857143em}.katex .fontsize-ensurer.reset-size3.size10,.katex .sizing.reset-size3.size10{font-size:2.96285714em}.katex .fontsize-ensurer.reset-size3.size11,.katex .sizing.reset-size3.size11{font-size:3.55428571em}.katex .fontsize-ensurer.reset-size4.size1,.katex .sizing.reset-size4.size1{font-size:.625em}.katex .fontsize-ensurer.reset-size4.size2,.katex .sizing.reset-size4.size2{font-size:.75em}.katex .fontsize-ensurer.reset-size4.size3,.katex .sizing.reset-size4.size3{font-size:.875em}.katex .fontsize-ensurer.reset-size4.size4,.katex .sizing.reset-size4.size4{font-size:1em}.katex .fontsize-ensurer.reset-size4.size5,.katex .sizing.reset-size4.size5{font-size:1.125em}.katex .fontsize-ensurer.reset-size4.size6,.katex .sizing.reset-size4.size6{font-size:1.25em}.katex .fontsize-ensurer.reset-size4.size7,.katex .sizing.reset-size4.size7{font-size:1.5em}.katex .fontsize-ensurer.reset-size4.size8,.katex .sizing.reset-size4.size8{font-size:1.8em}.katex .fontsize-ensurer.reset-size4.size9,.katex .sizing.reset-size4.size9{font-size:2.16em}.katex .fontsize-ensurer.reset-size4.size10,.katex .sizing.reset-size4.size10{font-size:2.5925em}.katex .fontsize-ensurer.reset-size4.size11,.katex .sizing.reset-size4.size11{font-size:3.11em}.katex .fontsize-ensurer.reset-size5.size1,.katex .sizing.reset-size5.size1{font-size:.55555556em}.katex .fontsize-ensurer.reset-size5.size2,.katex .sizing.reset-size5.size2{font-size:.66666667em}.katex .fontsize-ensurer.reset-size5.size3,.katex .sizing.reset-size5.size3{font-size:.77777778em}.katex .fontsize-ensurer.reset-size5.size4,.katex .sizing.reset-size5.size4{font-size:.88888889em}.katex .fontsize-ensurer.reset-size5.size5,.katex .sizing.reset-size5.size5{font-size:1em}.katex .fontsize-ensurer.reset-size5.size6,.katex .sizing.reset-size5.size6{font-size:1.11111111em}.katex .fontsize-ensurer.reset-size5.size7,.katex .sizing.reset-size5.size7{font-size:1.33333333em}.katex .fontsize-ensurer.reset-size5.size8,.katex .sizing.reset-size5.size8{font-size:1.6em}.katex .fontsize-ensurer.reset-size5.size9,.katex .sizing.reset-size5.size9{font-size:1.92em}.katex .fontsize-ensurer.reset-size5.size10,.katex .sizing.reset-size5.size10{font-size:2.30444444em}.katex .fontsize-ensurer.reset-size5.size11,.katex .sizing.reset-size5.size11{font-size:2.76444444em}.katex .fontsize-ensurer.reset-size6.size1,.katex .sizing.reset-size6.size1{font-size:.5em}.katex .fontsize-ensurer.reset-size6.size2,.katex .sizing.reset-size6.size2{font-size:.6em}.katex .fontsize-ensurer.reset-size6.size3,.katex .sizing.reset-size6.size3{font-size:.7em}.katex .fontsize-ensurer.reset-size6.size4,.katex .sizing.reset-size6.size4{font-size:.8em}.katex .fontsize-ensurer.reset-size6.size5,.katex .sizing.reset-size6.size5{font-size:.9em}.katex .fontsize-ensurer.reset-size6.size6,.katex .sizing.reset-size6.size6{font-size:1em}.katex .fontsize-ensurer.reset-size6.size7,.katex .sizing.reset-size6.size7{font-size:1.2em}.katex .fontsize-ensurer.reset-size6.size8,.katex .sizing.reset-size6.size8{font-size:1.44em}.katex .fontsize-ensurer.reset-size6.size9,.katex .sizing.reset-size6.size9{font-size:1.728em}.katex .fontsize-ensurer.reset-size6.size10,.katex .sizing.reset-size6.size10{font-size:2.074em}.katex .fontsize-ensurer.reset-size6.size11,.katex .sizing.reset-size6.size11{font-size:2.488em}.katex .fontsize-ensurer.reset-size7.size1,.katex .sizing.reset-size7.size1{font-size:.41666667em}.katex .fontsize-ensurer.reset-size7.size2,.katex .sizing.reset-size7.size2{font-size:.5em}.katex .fontsize-ensurer.reset-size7.size3,.katex .sizing.reset-size7.size3{font-size:.58333333em}.katex .fontsize-ensurer.reset-size7.size4,.katex .sizing.reset-size7.size4{font-size:.66666667em}.katex .fontsize-ensurer.reset-size7.size5,.katex .sizing.reset-size7.size5{font-size:.75em}.katex .fontsize-ensurer.reset-size7.size6,.katex .sizing.reset-size7.size6{font-size:.83333333em}.katex .fontsize-ensurer.reset-size7.size7,.katex .sizing.reset-size7.size7{font-size:1em}.katex .fontsize-ensurer.reset-size7.size8,.katex .sizing.reset-size7.size8{font-size:1.2em}.katex .fontsize-ensurer.reset-size7.size9,.katex .sizing.reset-size7.size9{font-size:1.44em}.katex .fontsize-ensurer.reset-size7.size10,.katex .sizing.reset-size7.size10{font-size:1.72833333em}.katex .fontsize-ensurer.reset-size7.size11,.katex .sizing.reset-size7.size11{font-size:2.07333333em}.katex .fontsize-ensurer.reset-size8.size1,.katex .sizing.reset-size8.size1{font-size:.34722222em}.katex .fontsize-ensurer.reset-size8.size2,.katex .sizing.reset-size8.size2{font-size:.41666667em}.katex .fontsize-ensurer.reset-size8.size3,.katex .sizing.reset-size8.size3{font-size:.48611111em}.katex .fontsize-ensurer.reset-size8.size4,.katex .sizing.reset-size8.size4{font-size:.55555556em}.katex .fontsize-ensurer.reset-size8.size5,.katex .sizing.reset-size8.size5{font-size:.625em}.katex .fontsize-ensurer.reset-size8.size6,.katex .sizing.reset-size8.size6{font-size:.69444444em}.katex .fontsize-ensurer.reset-size8.size7,.katex .sizing.reset-size8.size7{font-size:.83333333em}.katex .fontsize-ensurer.reset-size8.size8,.katex .sizing.reset-size8.size8{font-size:1em}.katex .fontsize-ensurer.reset-size8.size9,.katex .sizing.reset-size8.size9{font-size:1.2em}.katex .fontsize-ensurer.reset-size8.size10,.katex .sizing.reset-size8.size10{font-size:1.44027778em}.katex .fontsize-ensurer.reset-size8.size11,.katex .sizing.reset-size8.size11{font-size:1.72777778em}.katex .fontsize-ensurer.reset-size9.size1,.katex .sizing.reset-size9.size1{font-size:.28935185em}.katex .fontsize-ensurer.reset-size9.size2,.katex .sizing.reset-size9.size2{font-size:.34722222em}.katex .fontsize-ensurer.reset-size9.size3,.katex .sizing.reset-size9.size3{font-size:.40509259em}.katex .fontsize-ensurer.reset-size9.size4,.katex .sizing.reset-size9.size4{font-size:.46296296em}.katex .fontsize-ensurer.reset-size9.size5,.katex .sizing.reset-size9.size5{font-size:.52083333em}.katex .fontsize-ensurer.reset-size9.size6,.katex .sizing.reset-size9.size6{font-size:.5787037em}.katex .fontsize-ensurer.reset-size9.size7,.katex .sizing.reset-size9.size7{font-size:.69444444em}.katex .fontsize-ensurer.reset-size9.size8,.katex .sizing.reset-size9.size8{font-size:.83333333em}.katex .fontsize-ensurer.reset-size9.size9,.katex .sizing.reset-size9.size9{font-size:1em}.katex .fontsize-ensurer.reset-size9.size10,.katex .sizing.reset-size9.size10{font-size:1.20023148em}.katex .fontsize-ensurer.reset-size9.size11,.katex .sizing.reset-size9.size11{font-size:1.43981481em}.katex .fontsize-ensurer.reset-size10.size1,.katex .sizing.reset-size10.size1{font-size:.24108004em}.katex .fontsize-ensurer.reset-size10.size2,.katex .sizing.reset-size10.size2{font-size:.28929605em}.katex .fontsize-ensurer.reset-size10.size3,.katex .sizing.reset-size10.size3{font-size:.33751205em}.katex .fontsize-ensurer.reset-size10.size4,.katex .sizing.reset-size10.size4{font-size:.38572806em}.katex .fontsize-ensurer.reset-size10.size5,.katex .sizing.reset-size10.size5{font-size:.43394407em}.katex .fontsize-ensurer.reset-size10.size6,.katex .sizing.reset-size10.size6{font-size:.48216008em}.katex .fontsize-ensurer.reset-size10.size7,.katex .sizing.reset-size10.size7{font-size:.57859209em}.katex .fontsize-ensurer.reset-size10.size8,.katex .sizing.reset-size10.size8{font-size:.69431051em}.katex .fontsize-ensurer.reset-size10.size9,.katex .sizing.reset-size10.size9{font-size:.83317261em}.katex .fontsize-ensurer.reset-size10.size10,.katex .sizing.reset-size10.size10{font-size:1em}.katex .fontsize-ensurer.reset-size10.size11,.katex .sizing.reset-size10.size11{font-size:1.19961427em}.katex .fontsize-ensurer.reset-size11.size1,.katex .sizing.reset-size11.size1{font-size:.20096463em}.katex .fontsize-ensurer.reset-size11.size2,.katex .sizing.reset-size11.size2{font-size:.24115756em}.katex .fontsize-ensurer.reset-size11.size3,.katex .sizing.reset-size11.size3{font-size:.28135048em}.katex .fontsize-ensurer.reset-size11.size4,.katex .sizing.reset-size11.size4{font-size:.32154341em}.katex .fontsize-ensurer.reset-size11.size5,.katex .sizing.reset-size11.size5{font-size:.36173633em}.katex .fontsize-ensurer.reset-size11.size6,.katex .sizing.reset-size11.size6{font-size:.40192926em}.katex .fontsize-ensurer.reset-size11.size7,.katex .sizing.reset-size11.size7{font-size:.48231511em}.katex .fontsize-ensurer.reset-size11.size8,.katex .sizing.reset-size11.size8{font-size:.57877814em}.katex .fontsize-ensurer.reset-size11.size9,.katex .sizing.reset-size11.size9{font-size:.69453376em}.katex .fontsize-ensurer.reset-size11.size10,.katex .sizing.reset-size11.size10{font-size:.83360129em}.katex .fontsize-ensurer.reset-size11.size11,.katex .sizing.reset-size11.size11{font-size:1em}.katex .delimsizing.size1{font-family:KaTeX_Size1}.katex .delimsizing.size2{font-family:KaTeX_Size2}.katex .delimsizing.size3{font-family:KaTeX_Size3}.katex .delimsizing.size4{font-family:KaTeX_Size4}.katex .delimsizing.mult .delim-size1>span{font-family:KaTeX_Size1}.katex .delimsizing.mult .delim-size4>span{font-family:KaTeX_Size4}.katex .nulldelimiter{display:inline-block;width:.12em}.katex .delimcenter,.katex .op-symbol{position:relative}.katex .op-symbol.small-op{font-family:KaTeX_Size1}.katex .op-symbol.large-op{font-family:KaTeX_Size2}.katex .accent>.vlist-t,.katex .op-limits>.vlist-t{text-align:center}.katex .accent .accent-body{position:relative}.katex .accent .accent-body:not(.accent-full){width:0}.katex .overlay{display:block}.katex .mtable .vertical-separator{display:inline-block;min-width:1px}.katex .mtable .arraycolsep{display:inline-block}.katex .mtable .col-align-c>.vlist-t{text-align:center}.katex .mtable .col-align-l>.vlist-t{text-align:left}.katex .mtable .col-align-r>.vlist-t{text-align:right}.katex .svg-align{text-align:left}.katex svg{fill:currentColor;stroke:currentColor;fill-rule:nonzero;fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1;display:block;height:inherit;position:absolute;width:100%}.katex svg path{stroke:none}.katex img{border-style:none;max-height:none;max-width:none;min-height:0;min-width:0}.katex .stretchy{display:block;overflow:hidden;position:relative;width:100%}.katex .stretchy:after,.katex .stretchy:before{content:""}.katex .hide-tail{overflow:hidden;position:relative;width:100%}.katex .halfarrow-left{left:0;overflow:hidden;position:absolute;width:50.2%}.katex .halfarrow-right{overflow:hidden;position:absolute;right:0;width:50.2%}.katex .brace-left{left:0;overflow:hidden;position:absolute;width:25.1%}.katex .brace-center{left:25%;overflow:hidden;position:absolute;width:50%}.katex .brace-right{overflow:hidden;position:absolute;right:0;width:25.1%}.katex .x-arrow-pad{padding:0 .5em}.katex .cd-arrow-pad{padding:0 .55556em 0 .27778em}.katex .mover,.katex .munder,.katex .x-arrow{text-align:center}.katex .boxpad{padding:0 .3em}.katex .fbox,.katex .fcolorbox{border:.04em solid;box-sizing:border-box}.katex .cancel-pad{padding:0 .2em}.katex .cancel-lap{margin-left:-.2em;margin-right:-.2em}.katex .sout{border-bottom-style:solid;border-bottom-width:.08em}.katex .angl{border-right:.049em solid;border-top:.049em solid;box-sizing:border-box;margin-right:.03889em}.katex .anglpad{padding:0 .03889em}.katex .eqn-num:before{content:"(" counter(katexEqnNo) ")";counter-increment:katexEqnNo}.katex .mml-eqn-num:before{content:"(" counter(mmlEqnNo) ")";counter-increment:mmlEqnNo}.katex .mtr-glue{width:50%}.katex .cd-vert-arrow{display:inline-block;position:relative}.katex .cd-label-left{display:inline-block;position:absolute;right:calc(50% + .3em);text-align:left}.katex .cd-label-right{display:inline-block;left:calc(50% + .3em);position:absolute;text-align:right}.katex-display{display:block;margin:1em 0;text-align:center}.katex-display>.katex{display:block;text-align:center;white-space:nowrap}.katex-display>.katex>.katex-html{display:block;position:relative}.katex-display>.katex>.katex-html>.tag{position:absolute;right:0}.katex-display.leqno>.katex>.katex-html>.tag{left:0;right:auto}.katex-display.fleqn>.katex{padding-left:2em;text-align:left}body{counter-reset:katexEqnNo mmlEqnNo}

/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.katex-error {
	color: var(--vscode-editorError-foreground);
}

</style>
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item { list-style-type: none; } .task-list-item-checkbox { margin-left: -20px; vertical-align: middle; }
</style>
        
    </head>
    <body class="vscode-body vscode-light">
        <h1 id="p1674r2-evolving-a-standard-c-linear-algebra-library-from-the-blas">P1674R2: Evolving a Standard C++ Linear Algebra Library from the BLAS</h1>
<h2 id="authors">Authors</h2>
<ul>
<li>Mark Hoemmen (<a href="mailto:mark.hoemmen@gmail.com">mark.hoemmen@gmail.com</a>) (NVIDIA)</li>
<li>D. S. Hollman (me@dsh.fyi) (Google)</li>
<li>Christian Trott (<a href="mailto:crtrott@sandia.gov">crtrott@sandia.gov</a>) (Sandia National Laboratories)</li>
</ul>
<h2 id="date-2022-05-15">Date: 2022-05-15</h2>
<h2 id="revision-history">Revision history</h2>
<ul>
<li>
<p>Revision 0 submitted 2019-06-17</p>
</li>
<li>
<p>Revision 1 to be submitted 2022-04-15</p>
<ul>
<li>
<p>Clean up Markdown formatting</p>
</li>
<li>
<p>Update links and references</p>
</li>
<li>
<p>Account for P2128 (multidimensional <code>operator[]</code>),
which WG21 adopted on 2021-10</p>
</li>
<li>
<p>Account for changes to P1673 and other proposals in flight</p>
</li>
</ul>
</li>
<li>
<p>Revision 2 to be submitted 2022-05-15</p>
<ul>
<li>
<p>To match the latest P1673 revision,
change language about Symmetric, Hermitian, and Triangular algorithms
from &quot;assuming&quot; things about the values in the not-accessed triangle,
to &quot;interpreting&quot; things about those values</p>
</li>
<li>
<p>Added new references and updated existing ones (thanks to Jeff Hammond)</p>
</li>
</ul>
</li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>Many different applications depend on linear algebra.  This includes
machine learning, data mining, web search, statistics, computer
graphics, medical imaging, geolocation and mapping, and physics-based
simulations.  Good implementations of seemingly trivial linear algebra
algorithms can perform asymptotically better than bad ones, and can
get much more accurate results.  This is why authors (not just us!)
have proposed adding linear algebra to the C++ Standard Library.</p>
<p>Linear algebra builds on over 40 years of well-accepted libraries, on
which science and engineering applications depend.  In fact, there is
an actual multiple-language standard for a set of fundamental
operations, called the
<a href="http://www.netlib.org/blas/blast-forum/blas-report.pdf">Basic Linear Algebra Subprograms (BLAS) Standard</a>.
The BLAS Standard defines a small set of hooks that vendors or experts
in computer architecture can optimize.  Application developers or
linear algebra algorithm experts can then build on these hooks to get
good performance for a variety of algorithms.  The only thing missing
from the BLAS is a modern C++ interface.  Nevertheless, its wide
availability and many highly optimized implementations make it a good
starting point for developing a standard C++ library.</p>
<p>In this paper, we will walk through the design iterations of a typical
C++ linear algebra library that builds on the BLAS.  Our point is to
show how developers might &quot;grow&quot; a C++ linear algebra library
incrementally, solving problems as encountered and gradually
increasing the level of abstraction.  This process will highlight
challenges that C++ developers using the BLAS currently face.  It will
also show features C++ developers need that the BLAS and its
derivatives, like the
<a href="http://www.netlib.org/lapack/">LAPACK Fortran library</a>,
do not provide.  Finally, it
will give our views about how simple a C++ &quot;BLAS wrapper&quot; could be and
still be useful.</p>
<p>This paper implicitly argues for inclusion of linear algebra in the
C++ Standard Library.  It is meant to be read as part of the design
justification for our C++ Standard Library proposal
<a href="https://wg21.link/p1673">P1673</a>,
&quot;A free function linear algebra interface based on the BLAS.&quot;</p>
<p>We base this work on our years of experience writing and using linear
algebra libraries, and working with people who have done so for much
longer than we have.  Readers may wish to refer to
<a href="http://wg21.link/p1417r0">P1417R0</a> for a history of linear algebra library
standardization.  P1417R0 cites first-hand histories of the
development of the BLAS, related libraries like LINPACK and LAPACK
that use the BLAS, and Matlab (which started as a teaching tool that
wrapped these libraries).</p>
<h2 id="wrapping-the-blas">Wrapping the BLAS</h2>
<p>Suppose that a developer wants to write an application in portable
C++, that needs to compute dense matrix-matrix products and some other
dense linear algebra operations efficiently.  They discover that
implementing matrix multiply with a naïve triply nested loop is slow
for their use cases, and want to try a library.</p>
<p>The C++ Standard Library currently lacks linear algebra operations.
However, our hypothetical C++ developer knows that the Basic Linear
Algebra Subroutines (BLAS) exists.  The BLAS is a standard -- not an
ISO standard like C++, but nevertheless agreed upon by discussion and
votes, by people from many institutions.  The BLAS has been used for
decades by many scientific and engineering libraries and applications.
Many vendors offer optimized implementations for their hardware.
Third parties have written their own optimized implementations from a
combination of well-understood first principles (Goto and van de Geijn
2008) and automatic performance tuning (Bilmes et al. 1996, Whaley et
al. 2001). All this makes the BLAS attractive for our developer.</p>
<p>The BLAS standard has both C and Fortran bindings, but the
<a href="http://www.netlib.org/blas/#_reference_blas_version_3_10_0">reference implementation</a>
comes only in Fortran.  It's also slow; for example,
its matrix-matrix multiply routine uses nearly the same triply nested
loops that a naïve developer would write.  The intent of the BLAS is
that users who care about performance find optimized implementations,
either by hardware vendors or by projects like
<a href="http://math-atlas.sourceforge.net/">ATLAS</a> (see also Whaley et al. 2001),
<a href="https://www.tacc.utexas.edu/research-development/tacc-software/gotoblas2">GotoBLAS</a>,
<a href="https://github.com/xianyi/OpenBLAS">OpenBLAS</a>,
or <a href="https://github.com/flame/blis">BLIS</a>.</p>
<p>Suppose that our developer has found an optimized implementation of
the BLAS, and they want to call some of its routines from C++.  Here
are the first two steps they might take.</p>
<ol>
<li>
<p>Access the BLAS library.</p>
</li>
<li>
<p>Write a generic BLAS wrapper in C++.</p>
</li>
</ol>
<h3 id="access-the-blas-library">Access the BLAS library</h3>
<p>&quot;Accessing&quot; the BLAS library means both linking to it, and figuring
out how to call it from C++.  For the latter, our developer may start
either with the BLAS' C binding, or with its Fortran binding.</p>
<h4 id="linking-to-the-library">Linking to the library</h4>
<p>Our developer has two options for the BLAS: its C binding, or its
Fortran binding.  In both cases, they must add the library or
libraries to their link line.  One implementation (Intel's Math Kernel
Library) offers so many options for this case that they offer a web site
(formerly <a href="https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor">here</a>,
now <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl-link-line-advisor.html">here</a>)
to generate the necessary link line.
The multiplicity of options comes in part from the vendor's interest
in supporting different architectures (32- or 64-bit), operating
systems, compilers, and link options.  However, the BLAS
implementation itself has options, like whether it uses
<a href="https://www.openmp.org/">OpenMP</a> or some other run-time environment
for parallelism inside, or whether it uses an LP64
(32-bit signed integer matrix and vector dimensions) or ILP64
(64-bit signed integer matrix and vector dimensions) interface.
In the best case, getting these wrong
could cause link-time errors.  Use of the wrong interface through
<code>extern &quot;C&quot;</code> declarations could cause run-time errors.</p>
<p>Some developers only need to support one platform, so they rarely need
to figure out how to link to the BLAS.  Other developers need to
support many different platforms, and get good performance on all of
them.  Such developers end up writing build system logic for helping
installers automatically detect and verify BLAS libraries.  The author
has some experience writing and maintaining such build system logic as
part of the <a href="https://github.com/trilinos/Trilinos">Trilinos</a> project.
Other projects' build system logic takes at least this much effort.
For example, CMake 3.23.1's
<a href="https://cmake.org/cmake/help/latest/module/FindBLAS.html"><code>FindBLAS</code> module</a> has
<a href="https://github.com/Kitware/CMake/blob/master/Modules/FindBLAS.cmake">1343 lines of CMake code</a>,
not counting imported modules.  Other build systems, like GNU
Autoconf, have BLAS detection with
<a href="http://git.savannah.gnu.org/gitweb/?p=autoconf-archive.git;a=blob_plain;f=m4/ax_blas.m4">comparable complexity</a>.
(This example just includes finding the BLAS library, not actually
deducing the ABI.)</p>
<p>Typical logic for deducing name mangling either tries to link with all
known name manglings until the link succeeds, or inspects symbol names
in the BLAS library.  Since the ABI for function arguments and return
value is not part of the mangled symbol name, some build logic must
actually try to run code that calls certain BLAS functions (e.g.,
complex dot product), and make sure that the code does not crash or
return wrong results.  This hinders cross compilation.  In general,
it's impossible to make these tests generic.  Developers normally just
have special cases for known platforms and BLAS ABI bugs.</p>
<h4 id="c-binding">C binding</h4>
<p>The C BLAS is not as widely available as Fortran BLAS, but many
vendors have optimized implementations of both, so requiring the C
BLAS is not unreasonable.  If they choose the C binding, its header
file will import many symbols into the global namespace.  Best
practice would be to write wrapper functions for the desired C BLAS
functions, and hide the C binding's header file include in a source
file.  This prevents otherwise inevitable collisions with the same
<code>extern &quot;C&quot;</code> declarations in applications or other libraries.</p>
<p>If our developer later wants LAPACK functionality, such as solving
linear systems, they will discover that there is no standard C LAPACK
binding.  Implementations that provide a C BLAS generally also allow
calling the Fortran BLAS from the same application, but developers
would still need a C or C++ interface to LAPACK.</p>
<p>One advantage of the C BLAS binding is that it works for both
row-major and column-major matrix data layouts.  Native
multidimensional arrays in C and C++ have row-major layout, but the
BLAS' Fortran binding only accepts column-major matrices.  If users
want code that works for both the Fortran and C BLAS interfaces, they
will need to use column-major matrices.  The Standard Library
currently has no way to represent column-major rank-2 arrays (but see
our <a href="https://wg21.link/p0009"><code>mdspan</code> (P0009)</a> and
<a href="https://wg21.link/p1684"><code>mdarray</code> (P1684)</a>
proposals).  This leads C++ developers to one of the following
solutions:</p>
<ol>
<li>
<p>Write C-style code for column-major indexing (e.g., <code>A[i + stride*j]</code>);</p>
</li>
<li>
<p>use nonstandard matrix or array classes that use column-major
storage; or</p>
</li>
<li>
<p>trick the BLAS into working with row-major matrices, by specifying
that every operation use the transpose of the input matrix.  (This
only works for real matrix element types, since BLAS functions have
no &quot;take the conjugate but not the transpose&quot; option.)</p>
</li>
</ol>
<p>C++ lacks multidimensional arrays with run-time dimensions, so even if
developers want to use row-major indexing, they would still need to
write C-style code or use a matrix or array class.  This takes away
some of the value of a BLAS that can support row-major matrices.</p>
<h4 id="fortran-binding">Fortran binding</h4>
<p>The experience of developers of large scientific libraries is that the
C interface is less often available than the Fortran interface.  For
example, the Reference BLAS only comes in a Fortran version.  Thus,
the most portable approach is to rely on Fortran.  If our developer
links against a Fortran BLAS, they will face the difficult task of
deducing the library's Fortran BLAS ABI.  This is just close enough to
the C ABI to lead innocent developers astray.  For example, Fortran
compilers mangle the names of functions in different ways, that depend
both on the system and on the compiler.  Mangled names may be
all-capitals or lower-case, and many have zero to two underscores
somewhere at the beginning or end of the mangled name.  The BLAS may
have been built with a different compiler than the system's Fortran
compiler (if one is available), and may thus have a different mangling
scheme.  Fortran interfaces pass all input and output arguments,
including integers, by the C ABI equivalent of pointer, unless
otherwise specified.  Fortran ABIs differ in whether they return
complex numbers on the stack (by value) or by initial output argument,
and how they pass strings, both of which come up in the BLAS.  We have
encountered and worked around other issues, including actual BLAS ABI
bugs.  For instance, functions that claim to return <code>float</code>
might actually return <code>double</code>, as
<a href="https://github.com/trilinos/Trilinos/blob/7bc4eb697c4ccd9fab1742661da3260a1069d41a/packages/teuchos/cmake/CheckBlasFloatReturnsDouble.cmake">this Trilinos CMake function</a>
attempts to detect.</p>
<p>We have written hundreds of lines of build system code to deduce the
BLAS ABI, have encountered all of these situations, and expect to
encounter novel ones in the future.  Other projects do the same.  If
our developer wants to use LAPACK as well as the BLAS, they will be
stuck with these problems.</p>
<p>In summary:</p>
<ol>
<li>
<p>Users may need to deduce the BLAS library's Fortran ABI.
There is no platform-independent way to do this.</p>
</li>
<li>
<p>Ignorance of some Fortran ABI issues (e.g., return of complex
values) could cause bugs when porting code to different hardware or
software.</p>
</li>
</ol>
<p>Once our developer has deduced the Fortran BLAS ABI, they will need to
do the following:</p>
<ol>
<li>
<p>Write <code>extern &quot;C&quot;</code> declarations for all the BLAS functions they
want to call, using the deduced ABI to mangle function names and
correctly handle return values (if applicable).</p>
</li>
<li>
<p>Hide the <code>extern &quot;C&quot;</code> declarations and write wrappers to avoid
polluting the global namespace and colliding with other libraries
or applications.  (It's likely that the kinds of large applications
that would want to use this BLAS wrapper would already have their
own well-hidden <code>extern &quot;C&quot;</code> declarations.  The authors have
encountered this issue multiple times.)</p>
</li>
<li>
<p>Change wrappers to pass read-only integers or scalars by value or
const reference, instead of by pointer, so that users can pass in
integer or scalar literals if desired.</p>
</li>
</ol>
<h3 id="generic-c-wrapper">Generic C++ wrapper</h3>
<p>Suppose that our developer has solved the problems of the above
section.  They have access to the BLAS in their source code, through a
wrapped C API.  They will next encounter three problems:</p>
<ul>
<li>
<p>this C API is not type safe;</p>
</li>
<li>
<p>it is not generic; and</p>
</li>
<li>
<p>it is tied to availability of an external BLAS library.</p>
</li>
</ul>
<p>Our developer's next step for solving these problems would be to write
a generic C++ wrapper, with a fall-back implementation for unsupported
matrix or vector element types, or if the BLAS library is not
available.</p>
<p>Developers who just want to compute one matrix-matrix multiply for
matrices of <code>double</code> might be satisfied with the BLAS' C binding.
However, if they want to use this interface throughout their code,
they will discover the following problems.  First, the interface is
not type safe.  For example, the C BLAS doesn't know about
<code>_Complex</code> (as in Standard C) or <code>std::complex</code> (as in Standard C++).
Instead, it takes pointers to complex numbers as <code>void*</code>.
This violates
<a href="https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#Ri-typed">C++ Core Guidelines I.4</a>:
&quot;Make interfaces precisely and strongly typed.&quot;
Second, the interface is not generic.  It only works for
four matrix or vector element types:
<code>float</code>, <code>double</code>, <code>std::complex&lt;float&gt;</code>, and <code>std::complex&lt;double&gt;</code>.
What if our developer wants
to compute with matrices of lower-precision or higher-precision
floating-point types, integers, or custom types?  Most BLAS operations
only need addition and multiplication, yet the BLAS does not work with
matrices of integers.  The C and Fortran 77 function names also depend
on the matrix or vector element type, which hinders developing generic
algorithms.  Fortran 95's generic BLAS functions,
while convenient for Fortran programmers,
would only create more ABI deduction problems for C++ developers.
Third, what if the BLAS is not
available?  Users might like a fall-back implementation,
even if it is slow,
as a way to remove an external library dependency
or to test an external BLAS.</p>
<p>The logical solution is to write a generic C++ interface to BLAS
operations.  &quot;Generic C++ interface&quot; means that users can call some
C++ function templated on the matrix or vector element type <code>T</code>, and
the implementation will either dispatch to the appropriate BLAS call
if <code>T</code> is one of the four types that the BLAS supports, or fall back
to a default implementation otherwise.  The fall-back implementation
can even replace the BLAS library.</p>
<p>Libraries like the BLAS and LAPACK were written to be as generic as
possible.  (See our paper <a href="http://wg21.link/p1417r0">P1417R0</a> for a
survey of their history and the authors' intentions.)  Thus, once one
has figured out the ABI issues, it's not too much more effort to write
a generic C++ wrapper.  For example,
<a href="https://github.com/trilinos/Trilinos">Trilinos</a> has one,
<code>Teuchos::BLAS</code>, that has served it for over 15 years with few
changes.</p>
<p>Some subtle issues remain.  For example, some corresponding interfaces
for real and complex numbers differ.  Our developer must decide
whether they should try to present a single interface for both, or
expose the differences.  We have not found this to be a major issue in
practice.</p>
<h2 id="introduce-c-data-structures-for-matrices--vectors">Introduce C++ data structures for matrices &amp; vectors</h2>
<p>Many developers may find the above solution satisfactory.  For
example, the <code>Teuchos::BLAS</code> class in
<a href="https://github.com/trilinos/Trilinos">Trilinos</a> is a generic C++
wrapper for the BLAS, as we described above.  It has seen use in
libraries and applications since the mid 2000's, with rare changes.
However, many developers are not familiar with the BLAS, and/or find
its interface alien to C++.  The BLAS has no encapsulation of matrices
or vectors; it uses raw pointers.  Its functions take a long list of
pointer, integer, and scalar arguments in a mysterious order.  Mixing
up the arguments can cause memory corruption, without the benefit of
debug bounds checking that C++ libraries offer.  Furthermore, users
may need to work with column-major data layouts, which are not
idiomatic to C++ and may thus cause bugs.  This suggests that the next
step is to develop C++ data structures for matrices and vectors, and
extend the above BLAS wrapper to use them.</p>
<h3 id="blas-routines-take-many-unencapsulated-arguments">BLAS routines take many, unencapsulated arguments</h3>
<p>The <code>extern &quot;C&quot;</code> version of the BLAS interface violates the following
C++ Core Guidelines:</p>
<ul>
<li>
<p><a href="https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#i23-keep-the-number-of-function-arguments-low">I.23</a>,
&quot;Keep the number of function arguments low,&quot; and</p>
</li>
<li>
<p><a href="https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#i24-avoid-adjacent-parameters-that-can-be-invoked-by-the-same-arguments-in-either-order-with-different-meaning">I.24</a>,
&quot;Avoid adjacent parameters that can be invoked by the same arguments in either order with different meaning.&quot;</p>
</li>
</ul>
<p>Its functions take a large number of
function arguments, and put together unrelated parameters of the same
type.  A big reason for this is that neither the C nor the Fortran
binding of the BLAS gives users a way to encapsulate a matrix or
vector in a single data structure.  The four BLAS routines for
matrix-matrix multiply all have the following form:</p>
<pre><code><code><div>xGEMM(TRANSA, TRANSB, M, N, K, ALPHA, A, LDA, B, LDB, BETA, C, LDC)
</div></code></code></pre>
<p>where the initial x is one of S, D, C, or Z, depending on the matrix
element type.  The arguments have the following types:</p>
<ul>
<li>
<p><code>TRANSA</code> and <code>TRANSB</code> are character arrays of length at least 1;</p>
</li>
<li>
<p><code>M</code>, <code>N</code>, <code>K</code>, <code>LDA</code>, <code>LDB</code>, and <code>LDC</code> are integers;</p>
</li>
<li>
<p><code>ALPHA</code> and <code>BETA</code> are scalar values of the same type as the matrix
element type, and</p>
</li>
<li>
<p><code>A</code>, <code>B</code>, and <code>C</code> are rank-2 Fortran arrays with run-time
dimensions.</p>
</li>
</ul>
<p>These routines can perform several different operations, which their
documentation represents as <code>C = alpha*op(A)*op(B) + beta*C</code>.  Here,</p>
<ul>
<li>
<p><code>C</code> is the input and output matrix;</p>
</li>
<li>
<p><code>A</code> and <code>B</code> are the two input matrices;</p>
</li>
<li>
<p><code>C</code> is <code>M</code> by <code>N</code>, <code>A</code> is <code>M</code> by <code>K</code>, and <code>B</code> is <code>K</code> by <code>N</code>; and</p>
</li>
<li>
<p><code>op(X)</code> represents <code>X</code>, the transpose of <code>X</code>, or the conjugate
transpose of <code>X</code>, depending on the corresponding <code>TRANSX</code> argument.
<code>TRANSA</code> and <code>TRANSB</code> need not be the same.  For real matrix element
types, the conjugate transpose and transpose mean the same thing.</p>
</li>
</ul>
<p>The BLAS has a consistent system for ordering these arguments, that a
careful read of the
<a href="http://www.netlib.org/blas/blasqr.pdf">BLAS Quick Reference chart</a>
should suggest:</p>
<ol>
<li>
<p>Character arrays that modify behavior, if any;</p>
</li>
<li>
<p>Matrix and/or vector dimensions; then</p>
</li>
<li>
<p>Constants and matrices / vectors, in the order in which they appear
in the right-hand side of the algebraic expression to evaluate.
After each matrix / vector comes its stride argument.</p>
</li>
</ol>
<p>However, most users are not familiar with the BLAS' conventions.  We
<em>are</em>, and nevertheless we have found it easy to mix up these
arguments.  Users need to read carefully to see which of M, N, and K
go with A, B, or C.  Should they reverse these dimensions if taking
the transpose of A or B?  In some cases, the BLAS will check errors
for you and report the first argument (by number) that is wrong.  (The
BLAS' error reporting has its own issues; see &quot;Error checking and
handling&quot; below.)  In other cases, the BLAS may crash or get the wrong
answer.  Since the BLAS is a C or Fortran library, whatever debug
bounds checking you have on your arrays won't help.  It may not have
been built with debug symbols, so run-time debuggers may not help.
Developers who haven't done a careful job wrapping the BLAS in a
type-safe interface will learn the hard way, for example if they mix
up the order of arguments in the <code>extern &quot;C&quot;</code> declarations and their
integers get bitwise reinterpreted as pointers.</p>
<h3 id="matrix-and-vector-data-structures">Matrix and vector data structures</h3>
<p>The BLAS takes matrices and vectors as raw pointers.  This violates
C++ Core Guidelines <a href="https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#i13-do-not-pass-an-array-as-a-single-pointer">I.13</a>:
&quot;Do not pass an array as a single pointer.&quot;
However, C++ does not currently provide a BLAS-compatible matrix data
structure, with column-major storage, including dimensions and stride
information.  Thus, C++ does not currently give us a standard way
<em>not</em> to break that rule.  The language's native two-dimensional
arrays cannot have run-time dimensions and also promise contiguous or
constant-stride (row-major or column-major) storage.  Other array-like
data structures in the C++ Standard Library are only one dimensional.
The <code>valarray</code> class and its slice classes were meant to serve as a
building block for vectors and matrices (see footnote in
<strong>[template.valarray.overview]</strong>).  <strong>[class.slice.overview]</strong> calls
<code>slice</code> &quot;a BLAS-like slice from an array.&quot;  However, none of these
classes can represent a matrix without additional information.
Also, they require using <code>valarray</code> for data storage,
rather than whatever container the application might like to use.</p>
<p>The lack of general multidimensional array data structures in C++ led
us to propose <code>mdspan</code> <a href="https://wg21.link/p0009">(P0009)</a>.
An <code>mdspan</code> with <code>layout_left</code> layout
can represent a view of a dense column-major matrix
with leading dimension (stride) equal to the number of rows.
The BLAS permits matrices with leading dimension
greater than the number of rows,
for which case one can use either <code>mdspan</code>'s <code>layout_stride</code> layout,
or the <code>layout_blas_general</code> proposed in <a href="https://wg21.link/p1673">P1673</a>.
Thus, <code>mdspan</code> can encapsulate
all the pointer, dimensions, and stride arguments to BLAS functions
that represent a view of a matrix.  Unlike <code>valarray</code> slices,
<code>mdspan</code> can even represent dimensions and strides as compile-time values.
More importantly, <code>mdspan</code> can view matrices and vectors
stored in the application's preferred container types.
Applications need not commit to a particular container type,
unlike with <code>valarray</code>.
Therefore, we think it makes sense to
use <code>mdspan</code> as the minimal common interface for many different
data structures to interact with libraries like the BLAS.</p>
<h3 id="blas-library-reports-user-errors-incompatibly-with-c">BLAS library reports user errors incompatibly with C++</h3>
<p>The BLAS library checks user input, and reports user errors in a way
that is incompatible with C++ error handling.  Section 1.8 of the BLAS
Standard requires error checking and reporting for the Fortran 95,
Fortran 77, and C bindings.  Checking looks at dimensions, strides,
and bandwidths.  Users of the Fortran Reference BLAS see this as
program exit in the <code>XERBLA</code> routine; users of an implementation
conformant with the BLAS Standard see this as program exit in
<code>BLAS_error</code> or <code>blas_error</code>.  There are a few issues with the BLAS'
error reporting approach.</p>
<ol>
<li>
<p>There is no way to recover from an error.</p>
</li>
<li>
<p>There is no stack unwinding.</p>
</li>
<li>
<p>Users can replace the default error handler at link time, but it
must &quot;print an informative error message describing the error,
and halt execution.&quot;  There is no way to return control to the
caller.</p>
</li>
<li>
<p>Fortran did not define an equivalent of &quot;exit with a nonzero error
code&quot; until version 2008 of the Standard.  Thus, programs that use
the BLAS' Fortran binding and invoke the BLAS error handler could
exit with no indication to an automated caller that something went
wrong.</p>
</li>
</ol>
<p>Developers writing their own C++ wrapper
around an existing Fortran or C BLAS implementation
have at least two options to address these issues.</p>
<ol>
<li>
<p>Detect user errors in the wrapper.
Try to catch all cases that the BLAS would catch,
and report them in a way friendly to C++ code.</p>
</li>
<li>
<p>Declare that user errors invoke undefined behavior, just like
passing the wrong pointers to <code>memcpy</code>.  Let the BLAS library
detect errors if it wants.</p>
</li>
</ol>
<p>The problem with Option 1 is that it adds overhead
by duplicating the BLAS library's existing error checks.
For <a href="https://wg21.link/p1673">our proposal P1673</a>,
we have chosen Option 2,
by defining user run-time errors as Precondition violations.
This would let a Standard Library author
quickly stand up an implementation
by wrapping an existing BLAS library.
Later, they can go back and modify the BLAS library
to change how it detects and handles errors.</p>
<h3 id="function-argument-aliasing-and-zero-scalar-multipliers">Function argument aliasing and zero scalar multipliers</h3>
<p>Summary:</p>
<ol>
<li>
<p>The BLAS Standard forbids aliasing any input (read-only) argument
with any output (write-only or read-and-write) argument.</p>
</li>
<li>
<p>The BLAS uses <code>INTENT(INOUT)</code> (read-and-write) arguments to express
&quot;updates&quot; to a vector or matrix.  By contrast, C++ Standard
algorithms like <code>transform</code> take input and output iterator ranges
as different parameters, but may let input and output ranges be the
same.</p>
</li>
<li>
<p>The BLAS uses the values of scalar multiplier arguments (&quot;alpha&quot; or
&quot;beta&quot;) of vectors or matrices at run time, to decide whether to
treat the vectors or matrices as write only.  This matters both for
performance and semantically, assuming IEEE floating-point
arithmetic.</p>
</li>
<li>
<p>We recommend separately, based on the category of BLAS function,
how to translate <code>INTENT(INOUT)</code> arguments into a C++ idiom:</p>
<p>a. For in-place triangular solve or triangular multiply, we
recommend translating the function to take separate input and
output arguments that shall not alias each other.</p>
<p>b. Else, if the BLAS function unconditionally updates (like
<code>xGER</code>), we recommend retaining read-and-write behavior for that
argument.</p>
<p>c. Else, if the BLAS function uses a scalar <code>beta</code> argument to
decide whether to read the output argument as well as write to
it (like <code>xGEMM</code>), we recommend providing two versions: a
write-only version (as if <code>beta</code> is zero), and a read-and-write
version (as if <code>beta</code> is nonzero).</p>
</li>
</ol>
<p>Both the BLAS Standard and the C++ Standard Library's algorithms
impose constraints on aliasing of their pointer / array arguments.
However, the BLAS Standard uses different language to express these
constraints, and has different interface assumptions.  In this
section, we summarize the BLAS' constraints in C++ Standard terms, and
explain how an idiomatic C++ interface would differ.</p>
<h4 id="aliasing-in-the-blas">Aliasing in the BLAS</h4>
<p>The BLAS Standard says that its functions do not permit any aliasing
of their arguments.  In order to understand this restriction, we must
look both at Fortran (the BLAS' &quot;native language&quot;), and at the
<code>INTENT</code> specifications of BLAS functions.  It turns out that this
restriction is no more onerous than that on the input and output
arguments of <code>copy</code> or <code>memcpy</code>.  Furthermore, <code>INTENT(INOUT)</code> permits
the BLAS to express linear algebra &quot;update&quot; operations in an idiom
that linear algebra experts find more natural.  This differs from the
idiom of C++ Standard Library algorithms like <code>transform</code>, that can be
used to perform update operations.</p>
<p>The <a href="https://wg5-fortran.org/">Fortran standard</a> rarely refers to
&quot;aliasing.&quot;  The proper Fortran term is <em>association</em>.  In C++ terms,
&quot;association&quot; means something like &quot;refers to.&quot;  For example, Fortran
uses the term <em>argument association</em> to describe the binding of the
caller's <em>actual argument</em>, to the corresponding <em>dummy argument</em>.
C++ calls the former an &quot;argument&quot; <strong>[defns.argument]</strong> -- what the
caller puts in the parentheses when calling the function -- and the
latter a &quot;parameter&quot; <strong>[defns.parameter]</strong> -- the thing inside the
function that gets the value.  (Fortran uses &quot;parameter&quot; in a
different way than C++, to refer to a named constant.)
<em>Sequence association</em> means (among other things) that an array argument can
&quot;point to a subset&quot; of another array, just like it can in C++.  We
omit details about array slices, but C++ developers can get a rough
idea of this behavior by thinking of Fortran arrays as pointers.</p>
<p>In the latest (2018) Fortran standard, Section 15.5.2.3 gives argument
association rules, and Section 19.5 defines the different kinds of
association.  Section 15.5.1.2 explains that &quot;[a]rgument association
can be sequence association.&quot;  This
<a href="https://stevelionel.com/drfortran/2009/03/31/doctor-fortran-in-ive-come-here-for-an-argument/">blog post</a>
by Steve Lionel explains argument association and aliasing in detail.</p>
<p>A first-order C++ approximation of Fortran's default behavior is &quot;pass
scalar values by reference -- nonconst unless declared <code>INTENT(IN)</code> --
and pass arrays by pointer.&quot;  Thus, aliasing rules matter even more in
Fortran than in C++.  Fortran only permits associating the same entity
with two different dummy arguments if the dummy arguments are both
explicitly marked read only, through the <code>INTENT(IN)</code> attribute.
Function arguments can have four different <code>INTENT</code>s:</p>
<ul>
<li><code>INTENT(IN)</code>, read only;</li>
<li><code>INTENT(OUT)</code>, write only;</li>
<li><code>INTENT(INOUT)</code>, read and write; or</li>
<li>unspecified, so the function's behavior defines the actual &quot;intent.&quot;</li>
</ul>
<p>For example, the BLAS' <code>xAXPY</code> function performs the vector sum
operation <code>Y = Y + ALPHA*X</code>.  It has the following arguments:</p>
<ul>
<li><code>ALPHA</code> (scalar) is <code>INTENT(IN)</code>,</li>
<li><code>X</code> (vector) is <code>INTENT(IN)</code>, and</li>
<li><code>Y</code> (vector) is <code>INTENT(INOUT)</code>.</li>
</ul>
<p>&quot;No aliasing&quot; here means that when users call <code>xAPXY</code>, they promise
that neither of their first two actual arguments aliases any elements
in the third actual argument.  Aliasing two <code>INTENT(IN)</code> arguments is
legal.  For instance, <code>xGEMM</code> (matrix-matrix multiply) permits its
<code>INTENT(IN)</code> arguments <code>A</code> and <code>B</code> to be the same, as long as they do
not alias the <code>INTENT(INOUT)</code> argument <code>C</code>.</p>
<h4 id="aliasing-in-c-standard-algorithms">Aliasing in C++ Standard algorithms</h4>
<p>The <code>transform</code> (see <strong>[alg.transform]</strong>) algorithm is a good analog
to updating functions like <code>xAXPY</code>.  <code>transform</code> does not take the C++
equivalent of an <code>INTENT(INOUT)</code> argument.  Instead, <code>transform</code> takes
input and output iterator ranges as separate arguments, but lets its
output range be equal to either input range.  If so, then <code>transform</code>
implements an &quot;update&quot; that accesses the output in read-and-write
fashion.  This is the C++ way of expressing a read-and-write argument
for update operations.</p>
<p>In general, each C++ Standard algorithm (see e.g.,
<strong>[alg.modifying.operations]</strong>) states its own constraints on its
input and output iterator ranges.  For example, in <strong>[alg.copy]</strong>,</p>
<ul>
<li>
<p>Three-argument <code>copy</code> requires that the output iterator <code>result</code>
&quot;shall not be in the range <code>[first, last)</code>&quot; (the input iterators).</p>
</li>
<li>
<p>The overload of <code>copy</code> that takes an <code>ExecutionPolicy</code> requires that
the &quot;ranges <code>[first, last)</code> and <code>[result, result + (last - first))</code>
shall not overlap.&quot;</p>
</li>
<li>
<p><code>copy_if</code> in general requires that the input range <code>[first, last)</code>
and the output range <code>[result, result + (last - first))</code> &quot;shall not
overlap.&quot;</p>
</li>
</ul>
<p>Note the mismatch between the BLAS and the C++ Standard Library.  The
BLAS has <code>INTENT(INOUT)</code> arguments to express the idea of &quot;an input
that is also an output.&quot;  C++ Standard Library algorithms that have
both input and output ranges take separate arguments for those ranges.
In some cases, the separate input and output arguments may refer to
the same ranges, but they are still separate arguments.</p>
<h4 id="read-and-write-access-is-idiomatic">Read-and-write access is idiomatic</h4>
<p>Many linear algebra algorithms assume read-and-write access.  For
example, Krylov subspace methods compute an update to an existing
solution or residual vector.  Cholesky, LU, and QR factorizations
apply a low-rank (outer product) update to a trailing matrix.  Most of
these algorithms have no risk of parallel race conditions, as long as
users follow the rule that <code>INTENT(INOUT)</code> arguments may not alias
<code>INTENT(IN)</code> arguments.</p>
<p>The exceptions in the BLAS are the triangular solves <code>xTRSM</code> and
<code>xTRMM</code>, in which the right-hand side vector(s) are <code>INTENT(INOUT)</code>
arguments that the algorithm overwrites with the solution vector(s) on
output.  In practice, the authors often need to keep the original
right-hand side vectors, and end up making a copy before the
triangular solve.  This interface also precludes parallel
implementations, since the BLAS is not allowed to allocate memory for
temporary copies.</p>
<h4 id="blas-has-special-access-rules-for-zero-scalar-prefactors">BLAS has special access rules for zero scalar prefactors</h4>
<p>BLAS functions have special access rules when their operations
multiply a vector or matrix by a scalar prefactor.  Whenever the
scalar prefactor is zero, the BLAS does not actually read the vector's
or matrix's entries.  For example, the <code>xGEMM</code> function performs the
matrix-matrix multiply update <code>C := alpha * A * B + beta * C</code>, where
<code>A</code>, <code>B</code>, and <code>C</code> are matrices and <code>alpha</code> and <code>beta</code> are scalars.  If
<code>alpha</code> is zero, the BLAS does not read <code>A</code> or <code>B</code> and treats that
entire term as zero.  The <code>C</code> argument has declared <code>INTENT(INOUT)</code>,
but if <code>beta</code> is zero, the BLAS does not read <code>C</code> and treats the <code>C</code>
argument as write only.  This is a run-time decision, based on the
value(s) of the scalar argument(s).</p>
<p>The point of this rule is so that &quot;multiplying by zero&quot; has the
expected result of dropping that term in a sum.  This rule matters
semantically; it is not just a performance optimization.  In IEEE
floating-point arithmetic, <code>0.0 * A[i,j]</code> is <code>NaN</code>, not zero, if
<code>A[i,j]</code> is <code>Inf</code> or <code>NaN</code>.  If users have not initialized an
<code>INTENT(INOUT)</code> argument, then it's possible that some of the
uninitialized values may be <code>Inf</code> or <code>NaN</code>.  Linear algebra algorithm
developers depend on this behavior.  For example, textbook
formulations of some Krylov subspace methods assume this rule for
<code>xAPXY</code>, as a way to avoid a special case for the first iteration
(where the input vector may not be initialized).</p>
<h4 id="overloading-is-more-idiomatically-c">Overloading is more idiomatically C++</h4>
<p>The above special access rule is not idiomatic C++ for the following
reasons:</p>
<ol>
<li>
<p>C++ standard algorithms should be generic, but the rule makes sense
only for special cases of a particular arithmetic system.</p>
</li>
<li>
<p>The rule forces a branch with a major behavior change based on
run-time input values.  This violates both the zero overhead
requirement, and the Single Responsibility Principle.</p>
</li>
</ol>
<p>For instance, when we implemented BLAS-like computational kernels in the
<a href="https://github.com/trilinos/Trilinos">Trilinos</a> project,
Reason 2 required us either to put a branch in the inner loop,
or to have an outer branch that calls into one of two separate kernels.
Neither has zero overhead,
especially if the vectors or matrices are very small.
Optimized BLAS implementations likely take the latter approach of
implementing two separate kernels, since they do not prioritize
performance for very small problems.</p>
<p>A more idiomatic C++ linear algebra library could express write-only
vs. read-and-write semantics by overloading.  This would remove the
dependence of semantics on possibly run-time scalar values, and it
would match the convention in <strong>[algorithms.requirements]</strong> that
&quot;[b]oth in-place and copying versions are provided for certain
algorithms.&quot;  For example, the library would have two overloads of
<code>xGEMM</code>:</p>
<ol>
<li>
<p>an overload that takes <code>C</code> as a strictly write-only argument and
performs the operation <code>C := alpha * A * B</code>, without regard for the
value of <code>alpha</code>; and</p>
</li>
<li>
<p>an overload that performs the operation <code>C := alpha * A * B + beta * D</code>,
permits <code>D</code> to be the same as <code>C</code> (compare to <code>transform</code>), and
does so without regard for the values of <code>alpha</code> and <code>beta</code>.</p>
</li>
</ol>
<p>This would have the side benefit of extending the set of operations
&quot;for free.&quot;  For example, the overloading approach would give users a
<code>xWAXPY</code> operation <code>W := alpha*X + Y</code> without adding a new function
name or increasing implementer effort.  (In our proposal P1673, we
show how <code>mdspan</code>'s accessor policy would let us remove scalar
arguments like <code>alpha</code> and <code>beta</code> as well.)</p>
<p>The disadvantage of this approach is that implementations could no
longer just call the existing BLAS interface directly.  They would
need to introduce run-time checks (beyond what the BLAS already does)
for <code>alpha = 0</code> or <code>beta = 0</code> cases.  This is one justification for
proposing the removal of these special cases if adding linear algebra
to the C++ standard library
(see our proposal <a href="https://wg21.link/p1673">P1673</a>).  BLAS
implementations (that some vendors write already) or other BLAS-like
libraries likely have internal functions for implementing the
different cases, in order to avoid branches in inner loops.  A
standard library written by vendors could access those internal
functions.  Furthermore, a C++ linear algebra library for vectors and
matrices with very small compile-time sizes would in any case not want
to have these run-time branches.</p>
<p>A C++ library could alternately say that any <code>Inf</code> or <code>NaN</code> values
(either in input arrays or as the intermediate result of computations,
like <code>A * B</code> in the <code>alpha * A * B</code> term) give implementation-defined
results.  However, this would make the library's specification
non-generic on the matrix element type, which would defeat our goal of
a generic linear algebra library.  Thus, we do not favor this
approach.</p>
<h4 id="unconditionally-read-and-write-arguments">Unconditionally read-and-write arguments</h4>
<p>Many BLAS functions have unconditionally read-and-write behavior for
their output arguments.  This includes</p>
<ol>
<li>
<p>Element-wise functions over vectors or matrices, like <code>xSCAL</code>
(vector scale: <code>x := alpha * x</code>) and <code>xAXPY</code> (vector update:
<code>y = alpha * x + y</code>);</p>
</li>
<li>
<p>rank-1 or rank-2 matrix update functions, like <code>xGER</code>;</p>
</li>
<li>
<p>in-place triangular matrix-vector multiply functions, like <code>xTRMV</code>
and <code>xTRMM</code>; and in-place triangular solve functions, like <code>xTRSV</code>
and <code>xTRSM</code>.</p>
</li>
</ol>
<p>We consider each of these separately.  First, any reasonable
implementation of <code>xSCAL</code> should behave like <code>transform</code>, in that it
should work whether or not the output and input vectors are the same
(as long as they do not partially overlap).  Similarly, an operation
<code>w := alpha*x + beta*y</code> that permits <code>w</code> to be the same as <code>x</code> or <code>y</code>
would behave like <code>transform</code>, and would cover all <code>xAXPY</code> use cases.
The same argument applies to any element-wise function.</p>
<p>Second, rank-1 or rank-2 matrix update functions are idiomatic to the
implementation of matrix factorizations, in particular for matrices
with a small number of columns (the &quot;panel&quot; case in LAPACK).  Users
normally want to update the matrix in place.  Furthermore, <em>not</em>
updating makes a performance mistake.  An outer product that
overwrites a matrix destroys sparsity of the outer product
representation.  Users are better off keeping the vector(s), instead
of forming their outer product explicitly.  Updating an already dense
matrix with an outer product does not destroy sparsity.  The C++
Standard offers <code>sort</code> as precedent for only including the in-place
version of an algorithm.</p>
<p>Third, the in-place triangular matrix functions cannot be made
parallel without overhead (e.g., allocating intermediate storage).
This means that, unlike most C++ Standard algorithms, they could not
accept <code>ExecutionPolicy</code> overloads for parallel execution -- not even
for non-threaded vectorization.  Furthermore, in practice, users often
need to keep the original right-hand side vectors when they do
triangular solves, so they end up making a copy of the input / output
vector(s) before calling the BLAS.  Thus, an idiomatic C++ library
would only include versions of these functions that take separate
input and output objects, and would forbid aliasing of input and
output.</p>
<h4 id="summary">Summary</h4>
<p>A C++ version of the BLAS that wants idiomatic C++ treatment of input
and output arguments may need to translate each BLAS function with
<code>INTENT(INOUT)</code> arguments separately.</p>
<ol>
<li>
<p>For in-place triangular solve or triangular multiply, the function
would take separate input and output arguments that do not alias
each other.</p>
</li>
<li>
<p>Else, if the BLAS function unconditionally updates (like <code>xGER</code>),
the corresponding C++ function would have read-and-write behavior
for that argument.</p>
</li>
<li>
<p>Else, if the BLAS function uses a scalar <code>beta</code> argument to decide
whether to read the output argument as well as write to it (like
<code>xGEMM</code>), the C++ library would provide two versions: a write-only
version (as if <code>beta</code> is zero), and a read-and-write version (as if
<code>beta</code> is nonzero).</p>
</li>
</ol>
<h3 id="support-for-the-blas-different-matrix-storage-formats">Support for the BLAS' different matrix storage formats</h3>
<p>Summary:</p>
<ol>
<li>
<p>The dense BLAS supports several different dense matrix &quot;types.&quot;
Type is a mixture of &quot;storage format&quot; (e.g., packed, banded) and
&quot;mathematical property&quot; (e.g., symmetric, Hermitian, triangular).</p>
</li>
<li>
<p>Some &quot;types&quot; can be expressed as custom <code>mdspan</code> layouts;
others do not.</p>
</li>
<li>
<p>Thus, a C++ BLAS wrapper cannot overload on matrix &quot;type&quot; simply by
overloading on <code>mdspan</code> specialization.  The wrapper must use
different function names, tags, or some other way to decide what
the matrix type is.</p>
</li>
</ol>
<h4 id="blas-dense-matrix-storage-formats">BLAS dense matrix storage formats</h4>
<p>The dense BLAS supports several different dense matrix &quot;types&quot;
(storage formats).  We list them here, along with the abbreviation
that BLAS function names use for each.</p>
<ul>
<li>
<p>General (GE): Either column major or row major (C binding only).</p>
</li>
<li>
<p>General Band (GB): Stored like General, but functions take two
additional integers, one for the upper band width, and one for the
lower band width.</p>
</li>
<li>
<p>Symmetric (SY): Stored like General,
but the algorithm may only access half of the matrix
(either the upper or lower triangle).
The algorithm interprets the other half as if the matrix is symmetric
(<code>A[i,j] == A[j,i]</code>).  Functions take an <code>UPLO</code> argument to decide
whether to access the matrix's upper or lower triangle.</p>
</li>
<li>
<p>Symmetric Band (SB): The combination of General Band and Symmetric.</p>
</li>
<li>
<p>Symmetric Packed (SP): Like Symmetric, but stores entries in a
contiguous column-major packed format.</p>
</li>
<li>
<p>Hermitian (HE): Like Symmetric, but with the Hermitian property
(the complex conjugate of <code>A[i,j]</code> equals <code>A[j,i]</code>) instead of
symmetry.  Symmetry and the Hermitian property only differ if the
matrix's element type is complex.</p>
</li>
<li>
<p>Hermitian Band (HB): The combination of General Band and Hermitian.</p>
</li>
<li>
<p>Hermitian Packed (SP): Like Symmetric Packed,
with the Hermitian property instead of symmetry.</p>
</li>
<li>
<p>Triangular (TR): Functions take an <code>UPLO</code> argument to decide whether
to access the matrix's upper or lower triangle, and a <code>DIAG</code>
argument to decide whether to access the matrix's diagonal.
If <code>DIAG</code> is <code>'U'</code>, the function will not access the matrix's diagonal,
and will interpret the diagonal's elements as all ones (a &quot;unit diagonal&quot;).
Both options are relevant for using the results
of an in-place LU matrix factorization.</p>
</li>
<li>
<p>Triangular Band (TB): The combination of General Band and
Triangular.</p>
</li>
<li>
<p>Triangular Packed (TP): Stores entries in a contiguous column-major
packed format, like Symmetric Packed.  BLAS functions take the same
<code>UPLO</code> and <code>DIAG</code> arguments as Triangular.</p>
</li>
</ul>
<h4 id="matrix-storage-format-is-not-equivalent-to-mdspan-layout">Matrix storage format is not equivalent to mdspan layout</h4>
<p>The BLAS' &quot;matrix types&quot; conflate three different things:</p>
<ol>
<li>
<p>the arrangement of matrix elements in memory -- the mapping from a
2-D index space (row and column indices) to a 1-D index space (the
underlying storage);</p>
</li>
<li>
<p>constraints on what entries of the matrix an algorithm may access --
e.g., only the upper or lower triangle, or only the entries within a
band -- and interpretation of the values of entries not accessed; and</p>
</li>
<li>
<p>mathematical properties of a matrix, like symmetric, Hermitian,
banded, or triangular.</p>
</li>
</ol>
<p>&quot;The arrangement of matrix elements in memory&quot; is exactly what
<code>mdspan</code>'s layout intends to express.  However, the layout
cannot properly express &quot;constraints on what entries of the
matrix an algorithm may access.&quot;  P0009 defines the &quot;domain&quot;
of an <code>mdspan</code> -- its set of valid multidimensional indices --
as the Cartesian product of the dimensions (&quot;extents,&quot; in P0009 terms).</p>
<p>This excludes the various Triangular formats, since they do not permit
access outside the triangle.  One could &quot;hack&quot; the layout, along with
a special <code>mdspan</code> accessor, to return zero values for read-only
access outside the triangle.  However, <code>mdspan</code> does not have a
way to express &quot;read-only access for some matrix elements, and
read-and-write access for other matrix elements.&quot;  The <code>mdspan</code> class
was meant to be a low-level multidimensional array, not a fully
generalizable matrix data structure.</p>
<p>Similarly, there is no way to define a mathematically Hermitian matrix
using <code>mdspan</code>'s layout and accessor.  The layout could reverse
row and column indices outside of a specific triangle; that is a way
to define a mathematically symmetric matrix.  However, there is no way
for the layout to tell the accessor that the accessor needs to take
the conjugate for accesses outside the triangle.  Perhaps the accessor
could reverse-engineer this from the 1-D index, but again, this is
out of <code>mdspan</code>'s scope.</p>
<p>A C++ linear algebra library has a few possibilities for dealing with
this.</p>
<ol>
<li>
<p>It could use the layout and accessor types in <code>mdspan</code> simply
as tags to indicate the matrix &quot;type.&quot;  Algorithms could specialize
on those tags.</p>
</li>
<li>
<p>It could introduce a hierarchy of higher-level classes for
representing linear algebra objects, use <code>mdspan</code> (or
something like it) underneath, and write algorithms to those
higher-level classes.</p>
</li>
<li>
<p>It could imitate the BLAS, by introducing different function names,
if the layouts and accessors do not sufficiently describe the
arguments.</p>
</li>
</ol>
<p>In our proposal <a href="https://wg21.link/p1673">P1673</a>, we take Approach 3.
Our view is that a
BLAS-like interface should be as low-level as possible.  If a
different library wants to implement a &quot;Matlab in C++,&quot; it could then
build on this low-level library.  We also do not want to pollute
<code>mdspan</code> -- a simple class meant to be easy for the compiler to
optimize -- with extra baggage for representing what amounts to sparse
matrices.</p>
<h4 id="blas-general-calls-for-a-new-mdspan-layout">BLAS General calls for a new mdspan layout</h4>
<p>All BLAS matrix types but the Packed types actually assume the same
layout as General.  In our proposal <a href="https://wg21.link/p1673">P1673</a>,
we call General's layout
<code>layout_blas_general</code>.  It includes both row-major and column-major
variants: <code>layout_blas_general&lt;row_major_t&gt;</code>, and
<code>layout_blas_general&lt;column_major_t&gt;</code>.</p>
<p><code>layout_blas_general</code> expresses a more general layout than
<code>layout_left</code> or <code>layout_right</code>, because it permits a stride between
columns resp. rows that is greater than the corresponding dimension.
This is why BLAS functions take an &quot;LDA&quot; (leading dimension of the
matrix A) argument separate from the dimensions of A.  However, these
layouts are slightly <em>less</em> general than <code>layout_stride</code>, because they
assume contiguous storage of columns resp. rows.</p>
<p>One could omit <code>layout_blas_general</code> and use <code>layout_stride</code> without
removing functionality.  However, the advantage of these layouts is
that subspans taken by many matrix algorithms preserve the layout type
(if the stride is a run-time value).  Many matrix algorithms work on
&quot;submatrices&quot; that are rank-2 subspans of contiguous rows and columns
of a &quot;parent&quot; matrix.  If the parent matrix is <code>layout_left</code>, then in
general, the submatrix is <code>layout_stride</code>, not <code>layout_left</code>.
However, if the parent matrix is <code>layout_blas_general&lt;column_major_t&gt;</code>
or <code>layout_blas_general&lt;row_major_t&gt;</code>, such submatrices always have
the same layout as their parent matrix.  Algorithms on submatrices may
thus always assume contiguous access along one dimension.</p>
<h2 id="taking-stock">Taking stock</h2>
<p>Thus far, we have outlined the development of a generic C++ &quot;BLAS
wrapper&quot; that uses <code>mdspan</code> for matrix and vector parameters.
The library could call into the
BLAS (C or Fortran) for layouts and data types for which that is
possible, and would have fall-back implementations for other layouts and data
types.  We have also gradually adapted BLAS idioms to C++.  For
example, we talked about imitating the C++ Standard Algorithms with
respect to function argument aliasing, and how that relates to the
<code>alpha=0</code> and <code>beta=0</code> special cases for BLAS functions like
matrix-matrix multiply.</p>
<p>Recall that we are arguing for inclusion of linear algebra in the C++
Standard Library.  If the above interface were in the Standard,
vendors could optimize it without much effort, just by calling their
existing BLAS implementation, at least for the matrix and vector
element types that the BLAS supports.  The C++ library would also give
vendors the opportunity to optimize for other element types, or even
to drop the external BLAS library requirement.  For example, it's
possible to write a portable implementation of dense matrix-matrix
multiply directly to a matrix abstraction like <code>mdspan</code>, and
still get performance approaching that of a fully optimized
vendor-implemented BLAS library.  That was already possible given the
state of C++ compiler optimization 20 years ago; see e.g., Siek and
Lumsdaine 1998.  The authors would be ecstatic to have a product like
this available in the Standard Library.</p>
<h2 id="some-remaining-performance-issues">Some remaining performance issues</h2>
<p>Some C++ developers using a BLAS wrapper would encounter performance
issues that relate to limitations in the design of the BLAS' interface
itself.  Here are three such issues:</p>
<ol>
<li>
<p>It is not optimized for tiny matrices and vectors.</p>
</li>
<li>
<p>It has no way for users to control composition of parallelism, such
as what parallelism policy to use, or to control nested
parallelism.</p>
</li>
<li>
<p>Implementations have no way to optimize across multiple linear
algebra operations.</p>
</li>
</ol>
<p>The C++ interface outlined above has the right hooks to resolve these
issues.  In summary:</p>
<ol>
<li>
<p>The interface already permits specializing algorithms for
<code>mdspan</code> with compile-time dimensions.  The <code>mdarray</code> container class
<a href="https://wg21.link/p1684">(P1684)</a> can eliminate any possible overhead
from creating a view of a small matrix or vector.
It also gives convenient value semantics for small matrices and vectors.</p>
</li>
<li>
<p>Like the C++ Standard Library's algorithms, an optional
<code>ExecutionPolicy&amp;&amp;</code> argument would be a hook to support parallel
execution and hierarchical parallelism,
analogous to the existing parallel standard algorithms.</p>
</li>
<li>
<p>Optimizing across multiple linear algebra operations is possible,
but adds complications.  We talk below about different ways to
solve this problem.  It's not clear whether general solutions
belong in the C++ Standard Library.</p>
</li>
</ol>
<h3 id="tiny-matrices-and-vectors">Tiny matrices and vectors</h3>
<p>&quot;Tiny&quot; could mean any of the following:</p>
<ul>
<li>
<p>It's cheaper to pass the object by value than by pointer.</p>
</li>
<li>
<p>Function call or error checking overhead is significant.</p>
</li>
<li>
<p>The objects fit in registers or cache; memory bandwidth no longer
limits performance.</p>
</li>
</ul>
<p>The BLAS interface is not optimal for solving tiny problems as fast as
possible, for the following reasons:</p>
<ol>
<li>
<p>The BLAS is an external library; it cannot be inlined (at least not
without costly interprocedural optimization).</p>
</li>
<li>
<p>The BLAS standard requires error checking and reporting (see
above).  For small problems, error checking might take more time
than actually solving the problem.</p>
</li>
<li>
<p>The BLAS takes arrays by pointer, with run-time dimensions.
Neither is optimal for very small matrices and vectors.</p>
</li>
<li>
<p>The BLAS only solves one problem at a time; it does not have a
&quot;batched&quot; interface for solving many small problems at a time.</p>
</li>
</ol>
<p>It turns out that our hypothetical C++ library has already laid the
groundwork for solving all these problems.  The C++ &quot;fall-back&quot;
implementation is a great start for inlining, skipping error checks,
and optimization.  The <code>mdspan</code> class permits compile-time dimensions, or
mixes of compile-time and run-time dimensions.  It would be natural to
specialize and optimize the C++ implementation for common combinations
of compile-time dimensions.</p>
<p>The <code>mdspan</code> class is a view (<strong>[views]</strong>).  Implementations
store a pointer.  Thus, it is not totally zero overhead for very small
matrices or vectors with compile-time dimensions.  A zero-overhead
solution would only store the <em>data</em> at run time, not a pointer to the
data; <code>std::array</code> is an example.  Furthermore, it's awkward to use
views for very small objects (see example in P1684).  Users of small
matrices and vectors often want to handle them by value.  For these
reasons, we propose <code>mdarray</code> (P1684), a container version of
<code>mdspan</code>.</p>
<p>Once we have C++ functions that take <code>mdspan</code>, it's not much
more work to overload them to accept other matrix and vector data
structures, like <code>mdarray</code>.  This would also help our developer
make their linear algebra library more like the C++ Standard Library,
in that its algorithms would be decoupled from particular data
structures.</p>
<p>The <code>mdspan</code> class also gives developers efficient ways to
represent batches of linear algebra problems with the same dimensions.
For example, one could store a batch of matrices as a rank-3
<code>mdspan</code>, where the leftmost dimension selects the matrix.  The
various layouts and possibility of writing a custom layout make it
easier to write efficient batched code.  For example, one could change
the layout to facilitate vectorization across matrix operations.  We
have some experience doing so in our
<a href="https://github.com/kokkos">Kokkos</a> and
<a href="https://github.com/trilinos/Trilinos">Trilinos</a> libraries.</p>
<h3 id="composition-of-parallelism">Composition of parallelism</h3>
<p>Our developer may want to write a thread-parallel application.  What
happens if their BLAS implementation is thread parallel as well?  This
introduces two possible problems:</p>
<ol>
<li>
<p>The BLAS' threads might fight with the application's threads, even
while the application is not calling the BLAS.</p>
</li>
<li>
<p>The application may need to call the BLAS inside of a
thread-parallel region.  It may want to specify a subset of the
computer's parallel hardware resources on which the BLAS should
run, or it may not care what the BLAS does, as long as it doesn't
make the calling parallel code slower.</p>
</li>
</ol>
<p>BLAS implementations may use their own thread parallelism inside their
library.  This may involve OpenMP (as with some BLAS implementations) or a custom
Pthreads-based back-end (as we have seen in earlier GotoBLAS
releases).  The only way to know is to read the implementation's
documentation, and it may not be easy to control what it does at run
time (e.g., how many threads it spins up).</p>
<p>What if our hypothetical developer wants to use thread parallelism in
their own code?  Their thread parallelism run-time system or
implementation might fight with the BLAS' threads, even if our
developer never calls the BLAS in a thread-parallel region.  For
example, one way to construct a thread pool is to pin threads to cores
and have each thread spin-lock until work arrives.  If both the BLAS
and our developer do that, the two thread pools will fight constantly
over resources.</p>
<p>In our experience, BLAS implementations that use OpenMP generally play
nicely with the caller's use of OpenMP, as long as the caller uses the
same compiler and links with the same version of the OpenMP run-time
library.  Even if the two thread pools play nicely together, what
happens if our developer calls the BLAS inside of a parallel region?
Intel's Math Kernel Library recognizes this by using the OpenMP API to
determine whether it is being called inside of a parallel region.  If
so, it reverts to sequential mode.  A smarter implementation could
instead farm out work to other threads, if it makes sense for good
performance.  However, solving this at a system level, without our
developer needing to change both their code and that of the BLAS
implementation, is hard.  For example, Pan 2010 shows how to nest
parallel libraries and share resources between them without changing
library code, but the approach is invasive in parallel programming
run-time environments.  &quot;Invasive&quot; means things like &quot;reimplement
Pthreads and OpenMP.&quot;  Application developers may not have that level
of control over the run-time environments they use.</p>
<p>Our developer could help by providing an optional execution policy
(see <strong>[execpol]</strong> in the C++ Standard) that tells the BLAS what
subset of parallel resources it may use.  This is a logical extension
of the C++ linear algebra interface we have been developing.  Just
like C++ Standard Algorithms, our developer's library could take
optional execution policies.  The optional execution policy parameter
would also serve as an extension point for an interface that supports
hierarchical parallelism (a &quot;team-level BLAS&quot;).  That could also help
with code that wants to solve many tiny linear algebra problems in
parallel.  This is the design choice we made in our linear algebra
library proposal, P1673.</p>
<h3 id="optimize-across-operations">Optimize across operations</h3>
<p>The functions in the BLAS only perform one linear algebra operation at
a time.  However, in many cases one can improve performance by doing
multiple operations at a time.  That would let the implementation fuse
loops and reuse more data, and would also amortize parallel region
launch overhead in a parallel implementation.</p>
<h4 id="provide-specialized-fused-kernels">Provide specialized fused kernels?</h4>
<p>One way to do this is simply to expand the set of operations in the
interface, to include more specialized &quot;fused kernels.&quot;  The BLAS
already does this; for example, matrix-vector multiply is equivalent
to a sequence of dot products.  BLAS 2 and 3 exist in part for this
reason.  The <code>xGEMM</code> routines fuse matrix-matrix multiply with matrix addition,
in part because this is exactly what LAPACK's LU factorization needs
for trailing matrix updates.</p>
<p>This approach can work well if the set of operations to optimize is
small.  (See e.g., Vuduc 2004.)  The opaque interface to fused kernels
gives developers complete freedom to optimize.  However, it also makes
the interface bigger and harder to understand.  Users may miss
optimization opportunities because they failed to read that last page
of documentation with the fancy fused kernels.  Thus, the more general
the intended audience for a linear algebra library, the less value
specialized fused kernels may have.</p>
<h4 id="expression-templates">Expression templates?</h4>
<p>Many linear algebra libraries use expression templates to optimize
sequences of linear algebra operations.  See
<a href="http://wg21.link/p1417r0">P1417R0</a> for an incomplete list.  Expression
templates are a way to implement lazy evaluation, in that they defer
evaluation of arithmetic until assignment.  This also lets them avoid
allocation of temporary results.  When used with arithmetic operator
overloading, expression templates give users a notation more like
mathematics, with good performance.  Some communities, like game
developers, strongly prefer arithmetic operator overloading for linear
algebra, so arithmetic operators on linear algebra objects strongly
suggest expression templates.</p>
<p>We will consider two examples of linear algebra expressions:</p>
<ol>
<li>
<p>Compute the 2-norm of <code>w</code>, where <code>w = alpha*x + beta*y + gamma*z</code>
(<code>x</code>, <code>y</code>, <code>z</code>, and <code>w</code> are vectors, and <code>alpha</code>, <code>beta</code>, and
<code>gamma</code> are scalar constants).</p>
</li>
<li>
<p>Compute <code>D = A*B*C</code>, where <code>A</code>, <code>B</code>, and <code>C</code> are matrices with
run-time dimensions, <code>operator*</code> indicates the matrix-matrix
product, and <code>B*C</code> has many more columns than <code>D</code>.</p>
</li>
</ol>
<h5 id="first-example-norm-of-a-sum-of-scaled-vectors">First example: Norm of a sum of scaled vectors</h5>
<p>The first example is a classic use case for expression templates.  As
long as all vectors exist and have the same length, one can write an
expression templates library that computes the entire expression in a
single loop, without allocating any temporary vectors.  In fact, one
could even skip storage of the intermediate vector w, if one only
wanted its norm.</p>
<p>This example may suggest to some readers that we don't even need this
expression templates library.  What if a future version of Ranges were
to accept parallel execution policies, and work with
<code>transform_reduce</code>?  Wouldn't that make a expression templates library
for linear algebra just a pinch of syntactic sugar?  Such readers
might go on to question the value of a BLAS 1 interface.  Why do we
need dot products and norms?</p>
<p>Computing norms and dot products accurately and without unwarranted
underflow or overflow is tricky.  Consider the norm of a vector whose
elements are just a little bit larger than the square root of the
overflow threshold, for example.  The reference implementation of the
BLAS does extra scaling in its 2-norm and dot product implementations,
in order to avoid unwarranted overflow.  Many BLAS 2 and 3 operations
are mathematically equivalent to a sequence of dot products, and thus
share these concerns.</p>
<p>Even if C++ already has the tools to implement something, if it's
tricky to implement well, that can justify separate standardization.
The C++ Standard Library already includes many &quot;mathematical special
functions&quot; (<strong>[sf.cmath]</strong>), like incomplete elliptic integrals,
Bessel functions, and other polynomials and functions named after
various mathematicians.  Special functions can be tricky to implement
robustly and accurately; there are very few developers who are experts
in this area.  We think that linear algebra operations are at least as
broadly useful, and in many cases significantly more so.</p>
<h5 id="second-example-matrix-triple-product">Second example: Matrix triple product</h5>
<p>The second example shows the limits of expression templates in
optimizing linear algebra expressions.  The compiler can't know the
optimal order to perform the matrix-matrix multiplications.  Even if
it knew the dimensions at compile time, optimizing the general problem
of a chain of matrix multiplications is a nontrivial algorithm that
could be expensive to evaluate at compile time.  In this case, if the
compiler evaluates <code>B*C</code> first, it would need to allocate temporary
storage, since <code>D</code> is not large enough.  Introducing <code>operator*</code> for
matrix-matrix multiplcation thus forces the implementation to have
storage for temporary results.  That's fine for some users.  Others
could reach for named-function alternatives to arithmetic operators,
to write lower-level code that does not allocate temporary storage.</p>
<p>When LAPACK moved from Fortran 77 to a subset of Fortran 90 in the
2000's, LAPACK developers polled users to ask if they wanted the
interface to allocate scratch space internally (using Fortran 90
features).  Users rejected this option; they preferred to manage their
own scratch space.  Thus, having the option of a lower-level interface
is important for a general user community.</p>
<h5 id="expression-rewriting-with-named-functions">Expression rewriting with named functions</h5>
<p>One way to avoid creation of temporaries in such expressions, is to
combine named functions with expression templates.  Such functions
would pass an optional &quot;continuation object&quot; into the next expression.
This approach would still enable high-level expression rewriting (like
Eigen or uBLAS).  It would also be agnostic to whether expression
rewriting happens at compile time or run time.  For example, for some
operations with large sparse matrices and dense vectors, it could make
sense to have a run-time expression rewriting engine.  Compile-time
expression templates might increase build time, so it would be helpful
to have the option not to do them.</p>
<p>For example, suppose that someone wants to evaluate the dot product of
two vectors <code>y</code> and <code>z</code>, where <code>z = A*x</code> for a matrix <code>A</code> and vector
<code>x</code>.  In Matlab, one would write <code>y'*(A*x)</code>.  In our hypothetical
named-function library, we could write something like this:</p>
<pre><code class="language-c++"><div><span class="hljs-keyword">auto</span> A_times_x = <span class="hljs-built_in">make_expression</span>(z, <span class="hljs-built_in">prod</span>(A, x));
<span class="hljs-keyword">double</span> result = <span class="hljs-built_in">dot</span>(y, A_times_x);
</div></code></pre>
<p>or perhaps like this instead:</p>
<pre><code class="language-c++"><div><span class="hljs-keyword">double</span> result = <span class="hljs-built_in">dot</span>(y, <span class="hljs-built_in">prod</span>(z, A, x));
</div></code></pre>
<p>The library would have the option to use <code>z</code> as temporary storage, but
would not be required to do so.</p>
<p>There are two issues with this approach.  First, users would need to
query the library and the particular expression to know whether they
need to allocate temporary storage.  Second, expression templates in
general have the issue of dangling references; see next section.</p>
<h5 id="the-dangling-references-problem">The dangling references problem</h5>
<p>Expression templates implementations may suffer from the
<em>dangling references problem</em>.  That is, if expressions do not share ownership
of their &quot;parent&quot; linear algebra objects' data, then if one permits
(say) a matrix expression to escape a scope delimiting the lifetime of
its parent matrix, the expression will have a dangling reference to
its parent.  This is why Eigen's documentation
<a href="https://eigen.tuxfamily.org/dox/TopicPitfalls.html">recommends against</a>
assigning linear algebra expressions to <code>auto</code> variables.  This is an old
problem: <code>valarray</code> has the same issue, if the implementation takes
the freedom afforded to it by the Standard to use expression
templates.</p>
<p>Copy elision makes it impossible for an expression type to tell
whether it's being returned from a scope.  Otherwise, the expression
type could do something to claim ownership.  One possible fix is for
linear algebra objects to have <code>shared_ptr</code>-like semantics, so that
expressions share ownership of the objects they use.  However, that
introduces overhead, especially for the case of tiny matrices and
vectors.  It's not clear how to solve this problem in general,
especially if linear algebra objects may have container semantics.</p>
<h5 id="the-aliasing-problem-and-opting-out-of-lazy-evaluation">The aliasing problem and opting out of lazy evaluation</h5>
<p>Eigen's documentation also addresses the issue of
<a href="http://eigen.tuxfamily.org/dox/TopicLazyEvaluation.html">aliasing and expression templates</a>,
with the following example.  Suppose that <code>A</code> is a matrix, and users
evaluate the matrix-matrix multiplication <code>A = A*A</code>.  The expression
<code>A = A + A</code> could work fine with expression templates, since matrix
addition has no dependencies across different output matrix elements.
Computing <code>A[i,j]</code> on the left-hand side only requires reading the
value <code>A[i,j]</code> on the right-hand side.  In contrast, for the matrix
multiplication <code>A = A*A</code>, computing <code>A[i,j]</code> on the left-hand side
requires <code>A[i,k]</code> and <code>A[k,j]</code> on the right-hand side, for all valid
<code>k</code>.  This makes it impossible to compute <code>A = A*A</code> in place
correctly; the implementation needs a temporary matrix in order to get
the right answer.  This is why the Eigen library's expressions have an
option to turn off lazy evaluation, and do so by default for some
kinds of expressions.  Furthermore, allocating a temporary result
and/or eager evaluation of subexpressions may be faster in some cases.</p>
<p>This is true not necessarily just for expressions whose computations
have significant reuse, like matrix-matrix multiply, but also for some
expressions that &quot;stream&quot; through the entries of vectors or matrices.
For example, fusing too many loops may thrash the cache or cause
register spilling, so deciding whether to evaluate eagerly or lazily
may require hardware-specific information (see e.g., Siek et
al. 2008).  It's possible to encode many such compilation decisions in
a pure C++ library with architecture-specific parameters.  See, e.g.,
<a href="http://downloads.tuxfamily.org/eigen/eigen_CGLibs_Giugno_Pisa_2013.pdf">this 2013 Eigen presentation</a>,
and &quot;Lessons learned from <code>Boost::uBlas</code> development&quot; in
<a href="http://wg21.link/p1417r0">P1417R0</a>.  However, this burdens the library with
higher implementation complexity and increased compilation time.
Library designers may prefer a simpler interface that excludes
expressions with these issues, and lets users decide where temporaries
get allocated.</p>
<h5 id="how-general-should-expression-rewriting-be">How general should expression rewriting be?</h5>
<p>How many users actually write applications or libraries that have a
large number of distinct, complicated linear algebra expressions, that
are fixed at compile time?  Would they be better served by specialized
source-to-source translation tools, like those described in Siek et
al. 2008?</p>
<h5 id="are-arithmetic-operators-even-a-good-idea">Are arithmetic operators even a good idea?</h5>
<p>Different application areas have different interpretations of &quot;matrix&quot;
vs. &quot;vector,&quot; row vs. column vectors, and especially <code>operator*</code> (dot?
outer? Hadamard? element-wise? Kronecker?).  This makes introducing
arithmetic operators for matrices and vectors a bit risky.  Many users
want this, but it's important to get agreement on what the various
operators mean.  A general linear algebra library might prefer to stay
out of this contentious debate.</p>
<h2 id="some-remaining-correctness-issues">Some remaining correctness issues</h2>
<ol>
<li>
<p>Does mixing precisions, real and complex, etc. do the right thing?</p>
</li>
<li>
<p>Some kinds of number types may call for a different interface.</p>
</li>
</ol>
<h3 id="mixing-precisions-in-linear-algebra-expressions">Mixing precisions in linear algebra expressions</h3>
<p>Suppose that a linear algebra library implements arithmetic operators
on matrices, and users write <code>A * B</code> for a matrix A of
<code>complex&lt;float&gt;</code>, and a matrix B of <code>double</code>.  What should the matrix
element type of the returned matrix be?
The accuracy-preserving choice would be <code>complex&lt;double&gt;</code>.
However, the <code>common_type</code> of <code>double</code> and <code>complex&lt;float&gt;</code>
does <em>not</em> preserve accuracy in this case, as it is <code>complex&lt;float&gt;</code>.
The most &quot;syntactically sensible&quot; choice would be
<code>decltype(declval&lt;complex&lt;float&gt;&gt;() * declval&lt;double&gt;())</code>.
However, this type does not exist, because C++ does not
define the multiplication operator for these two types.</p>
<p>This issue may arise by accident.
It's easy to get a <code>double</code> without meaning to,
for example by using literal constants like 1.0.
Implementers of mixed-precision libraries
also need to watch out for bugs that silently reduce precision.  See
e.g., <a href="https://github.com/kokkos/kokkos-kernels/issues/101">this issue</a>
in a library to which we have contributed.
Internal expressions in implementations may need to deduce
extended-precision types for intermediate values.  For example, a
double-precision expression may want to reach for
a 128-bit floating-point type in order to reduce rounding error.
This is especially important for the shorter floating-point types
that people want to use in machine learning.
This may call for traits machinery that doesn't exist
in the C++ Standard Library yet.</p>
<p>There's no general solution to this problem.  Both users and
implementers of mixed-precision linear algebra need to watch out.
The C++ interface we propose in <a href="https://wg21.link/p1673">P1673</a>
avoids the issue of needing to deduce a return element type,
because users must explicitly specify the types of output matrices and vectors.
This means that users can do computations in extended precision,
just by specifying an output type with higher precision than the input types.
Implementations will still need to decide
whether to use higher precision internally for intermediate sums.</p>
<h3 id="different-interface-for-some-kinds-of-number-types">Different interface for some kinds of number types?</h3>
<p>Given a generic linear algebra library, users will put all sorts of
number types into it.  However, the BLAS was designed for real and
complex floating-point numbers.  Does a BLAS-like interface make sense
for matrices of fixed-point numbers?  Should such matrices carry along
scaling factors, for example?  What about &quot;short&quot; floating-point types
that exclude <code>Inf</code> and <code>NaN</code> to make room for more finite numbers?
Would that call for linear algebra operations that track underflow and
overflow?  How would such interfaces integrate with C++ versions of
libraries like LAPACK?  We are not experts on fixed-point arithmetic;
for us, these are all open questions.</p>
<h2 id="summary-1">Summary</h2>
<p>We started with a BLAS library, wrapped it in a C++ interface, and
gradually adapted the interface for a better fit to C++ idioms.
Without much effort, this development process fixed some serious
performance and correctness issues that can arise when calling the
BLAS from C++.  Our paper P1673 proposes a linear algebra library
for the C++ Standard that takes this approach.  Even if such a library
never enters the Standard, we think this style of interface is useful.</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Sandia National Laboratories is a multimission laboratory managed and
operated by National Technology &amp; Engineering Solutions of Sandia,
LLC, a wholly owned subsidiary of Honeywell International, Inc., for
the U.S. Department of Energy’s National Nuclear Security
Administration under contract DE-NA0003525.</p>
<p>Thanks to Damien Lebrun-Grandie for reviewing Revision 1 changes.</p>
<h2 id="references">References</h2>
<ul>
<li>
<p>J. Bilmes, K. Asanovíc, J. Demmel, D. Lam, and C. W. Chin, &quot;PHiPAC:
A Portable, High-Performance, ANSI C Coding Methodology and its
application to Matrix Multiply,&quot; LAPACK Working Note 111, 1996.</p>
</li>
<li>
<p>K. Goto and R. A. van de Geijn,
<a href="https://doi.org/10.1145/1356052.1356053">&quot;Anatomy of high-performance matrix multiplication&quot;</a>,
<em>ACM Transactions of Mathematical Software</em> (TOMS),
Vol. 34, No. 3, May 2008.  See also</p>
</li>
<li>
<p>M. Hoemmen, D. Hollman, C. Trott, D. Sunderland, N. Liber, A. Klinvex,
Li-Ta Lo, D. Lebrun-Grandie, G. Lopez, P. Caday, S. Knepper, P. Luszczek,
and T. Costa,
&quot;A free function linear algebra interface based on the BLAS,&quot;
P1673R7, Apr. 2022.</p>
</li>
<li>
<p>C. Trott, D. Hollman, M. Hoemmen, and D. Sunderland,
&quot;<code>mdarray</code>: An Owning Multidimensional Array Analog of <code>mdspan</code>&quot;,
P1684R1,
Mar. 2022.</p>
</li>
<li>
<p>Heidi Pan,
<a href="http://lithe.eecs.berkeley.edu/pubs/pan-phd-thesis.pdf">&quot;Cooperative Hierarchical Resource Management for Efficient Composition of Parallel Software,&quot;</a>,
PhD dissertation, Department of Electrical Engineering and Computer
Science, Massachusetts Institute of Technology, Jun. 2010.</p>
</li>
<li>
<p>J. Siek, I. Karlin, and E. Jessup, &quot;Build to order linear algebra
kernels,&quot; in Proceedings of International Symposium on Parallel and
Distributed Processing (IPDPS) 2008, pp. 1-8.</p>
</li>
<li>
<p>J. Siek and A. Lumsdaine, &quot;The Matrix Template Library: A Generic
Programming Approach to High Performance Numerical Linear Algebra,&quot;
in Proceedings of the Second International Symposium on Computing in
Object-Oriented Parallel Environments (ISCOPE) 1998, Santa Fe, NM,
USA, Dec. 1998.</p>
</li>
<li>
<p>F. G. Van Zee and R. A. van de Geijn,
<a href="https://doi.org/10.1145/2764454">&quot;BLIS: A Framework for Rapidly Instantiating BLAS Functionality,&quot;</a>,
<em>ACM Transactions on Mathematical Software</em> (TOMS), Vol. 41, No. 3, June 2015.</p>
</li>
<li>
<p>R. Vuduc, &quot;Automatic performance tuning of sparse matrix kernels,&quot;
PhD dissertation, Electrical Engineering and Computer Science,
University of California Berkeley, 2004.</p>
</li>
<li>
<p>R. C. Whaley, A. Petitet, and J. Dongarra, &quot;Automated Empirical
Optimization of Software and the ATLAS Project,&quot; <em>Parallel Computing</em>,
Vol. 27, No. 1-2, Jan. 2001, pp. 3-35.</p>
</li>
</ul>

        
        
    </body>
    </html>